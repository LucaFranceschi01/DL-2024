{"cells":[{"cell_type":"markdown","metadata":{"id":"8Xm5dMeYtIGr"},"source":["# Practice 2"]},{"cell_type":"markdown","metadata":{"id":"I16ZWRTGtIGt"},"source":["Student 1: Alejandro González Álvarez\n","\n","NIA 1: 252658\n","\n","Student 2: Luca Franceschi\n","\n","NIA 2: 253885\n","\n","Student 3: Júlia Othats-Dalès\n","\n","NIA 3: 254435"]},{"cell_type":"markdown","metadata":{"id":"fobsVjsgtIGu"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11575,"status":"ok","timestamp":1714942050764,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"-Sw4oLMotIGu"},"outputs":[],"source":["import torch\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from torch import nn\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"]},{"cell_type":"markdown","metadata":{"id":"sIT8YLYLtIGw"},"source":["# Google Drive (or not)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28508,"status":"ok","timestamp":1714942470974,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"k-n6KTZbtIGw","outputId":"3650eccc-0ef0-4333-b432-0d0decdd7bf4"},"outputs":[],"source":["# Comment if not in Google Colab\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# datadir = '/content/drive/My Drive/DeepLearning_2024/P2/Data/'\n","datadir = 'Data/'"]},{"cell_type":"markdown","metadata":{"id":"Yi3j82_JtIGw"},"source":["# GPU Acceleration (or not)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1714942050764,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"1_cMFpuotIGw","outputId":"5822977e-7aa8-4cb4-dfef-2de7441ce72a"},"outputs":[],"source":["# Print if gpu acceleration is enabled\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","print(f\"Using {device} device\")"]},{"cell_type":"markdown","metadata":{"id":"G1WSF05s1023"},"source":["# Ex.1 - Experimenting with Multi-Class Sequence Classification using RNNs and LSTMs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1714928614180,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"5bAW1k6R1vgk","outputId":"bb5a60fc-147b-438f-bd31-ebf038e17d3d"},"outputs":[],"source":["'''\n","Experimenting with Multi-Class Sequence Classification using RNNs and LSTMs\n","Steps :\n","1. Load and visualize the training and testing data in 'data/P2_E1.csv' as shown in the following code cell.\n","2. Following section 2 in the examples, train a RNN or LSTM to solve the multi-class sequence classification problem:\n","  -   Adapt the classification loss and the SequenceClassifier module\n","  -   Adapt the test_sequence_classifier function to compute the multi-class accuracy and be able to visualize the confusion matrix\n","3. Experiment with different models by changing different hyper-parameters (e.g, num_layers, hidden_size, optimiziers, activation_functions for RNNs, etc..) and evaluate  the results for each of them on the testing set.\n","4. Visualize analyse and discuss the results in the report.\n","'''"]},{"cell_type":"markdown","metadata":{"id":"cJoitFCf-9F1"},"source":["# Sol 1."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3481,"status":"ok","timestamp":1714928618004,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"TEF-ZsLvYDMR","outputId":"221380e8-69e8-4150-b309-9ae64e6dc80e"},"outputs":[],"source":["# 1. Load and visualize the training and testing data\n","data = np.load(datadir+'P2_E1.npz')\n","X_train, X_test = data['X_train'], data['X_test']\n","Y_train, Y_test = data['Y_train'], data['Y_test']\n","\n","print(f'Train Seqs: {X_train.shape}')\n","print(f'Train Labels: {Y_train.shape}')\n","print(f'Test Seqs: {X_test.shape}')\n","print(f'Test Labels: {Y_test.shape}')\n","\n","classes = np.unique(Y_train)\n","n_classes = len(classes)\n","colors = plt.cm.rainbow(np.linspace(0, 1, n_classes))\n","\n","fig, axs = plt.subplots(n_classes, 1, figsize=(10, 5*n_classes))\n","for class_idx in range(0, n_classes):\n","    axs[class_idx].plot(X_test[Y_test == class_idx, :].T, '-o', c=colors[class_idx])\n","    axs[class_idx].set_title(f'Tests Seqs. - Class {class_idx}')\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":282,"status":"ok","timestamp":1714928619489,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"LTcDZxdktIGy"},"outputs":[],"source":["# Define module encapsulating a Sequence Classifier using RNN or LSTMs and setting different architecture hyper-parameters\n","\n","class SequenceClassifier(nn.Module):\n","    def __init__(self,\n","                 input_size : int = 1,\n","                 hidden_size : int = 5,\n","                 num_layers = 1,\n","                 num_classes : int = 6,\n","                 use_lstm : bool = False):\n","        # Define RNN or LSTM architecture\n","        super().__init__()\n","        self.use_lstm = use_lstm\n","        if(use_lstm):\n","            self.rnn = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n","                               num_layers=num_layers, batch_first = True)\n","        else:\n","            self.rnn = nn.RNN(input_size = input_size, hidden_size = hidden_size,\n","                              num_layers=num_layers, batch_first = True)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.last_linear = nn.Linear(hidden_size, num_classes)  # 6 is number of different classes\n","\n","    def forward(self, X):\n","        _, last_states = self.rnn(X)\n","        # Get last hidden state for last layer. Ignore cell state in case of LSTMs\n","        if(not self.use_lstm):\n","            last_hidden_state = last_states[-1,:,:].squeeze(0)\n","        else:\n","            last_hidden_state = last_states[0][-1,:,:].squeeze(0)\n","        # Get sequence label probability using the last hidden state\n","        output = self.softmax(self.last_linear(last_hidden_state))  # Using softmax instead of sigmoid\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714928620654,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"WOLnmGw7tIGy"},"outputs":[],"source":["# Define the training model\n","def train_multiclass_classifier(X_train, Y_train, classifier, optimizer, loss_func, epochs=100):\n","    loss_epochs = []\n","    for epoch in range(epochs):\n","        optimizer.zero_grad()\n","        output = classifier(X_train)\n","        loss = loss_func(output, Y_train)\n","        loss_epochs.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","    print(f'Final loss: {loss.item()}')\n","    return np.asarray(loss_epochs)\n","\n","# Adapt the test_sequence_classifier function\n","def test_multiclass_classifier(X_test, Y_test, classifier):\n","    output = classifier(X_test)\n","    predicted_labels = torch.argmax(output, dim=1)\n","    correct_predictions = (predicted_labels == Y_test).sum().item()\n","    total_samples = len(Y_test)\n","    accuracy = correct_predictions / total_samples\n","    print(f'Test Accuracy: {accuracy:.4f}')\n","\n","    # Compute confusion matrix\n","    conf_matrix = confusion_matrix(Y_test.to(device), predicted_labels.to(device))\n","\n","    return accuracy, conf_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"executionInfo":{"elapsed":2027,"status":"error","timestamp":1714928729774,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"vvQxdimjtIGy","outputId":"2e5fc85a-ea89-41bd-f327-c1eeb2a4bcae"},"outputs":[],"source":["X_train_pt = torch.from_numpy(X_train).float().unsqueeze(2).to(device)\n","Y_train_pt = torch.from_numpy(Y_train).long().to(device)\n","X_test_pt = torch.from_numpy(X_test).float().unsqueeze(2).to(device)\n","Y_test_pt = torch.from_numpy(Y_test).long().to(device)\n","\n","loss_func = nn.CrossEntropyLoss()\n","\n","exp_hidden_size = [1, 5]\n","exp_num_layers = [1, 2]\n","exp_use_lstm = [False, True]\n","\n","losses_models = {}\n","test_accuracy_models = {}\n","confusion_matrices = {}\n","\n","for hidden_size, num_layers, use_lstm in zip(exp_hidden_size, exp_num_layers, exp_use_lstm):\n","    model_id = f'H{hidden_size}_NL{num_layers}_LSTM{int(use_lstm)}'\n","    print(f'Training: {model_id}')\n","\n","    seq_classifier = SequenceClassifier(use_lstm=use_lstm, num_layers=num_layers, hidden_size=hidden_size)\n","    seq_classifier.to(device)\n","\n","    optimizer = torch.optim.Adam(seq_classifier.parameters(), lr=1e-3)\n","\n","    losses_models[model_id] = train_multiclass_classifier(X_train_pt, Y_train_pt,\n","                                                          seq_classifier, optimizer,\n","                                                          loss_func, epochs=1000)\n","\n","    test_accuracy_models[model_id], confusion_matrices[model_id] = test_multiclass_classifier(X_test_pt, Y_test_pt, seq_classifier)\n","\n","#Plot confusion matrices\n","f, axes = plt.subplots(1, 2, figsize=(20, 20))\n","\n","a = [axes[0], axes[1]]\n","ctr = 0\n","for model, cm in confusion_matrices.items():\n","    ConfusionMatrixDisplay(cm).plot(include_values=True, cmap=\"Blues\", ax=a[ctr], colorbar=False, values_format=\".0f\")\n","    a[ctr].set_title(\"Model \" + str(ctr), size=16)\n","    ctr += 1\n","\n","plt.show()\n","\n","for loss_it in losses_models.values():\n","    plt.plot(loss_it)\n","plt.legend(losses_models.keys())\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Loss (J)\")\n","plt.show()\n","\n","pd.DataFrame(np.asarray(list(test_accuracy_models.values()))*100, columns=['accuracy %'],\n","             index=test_accuracy_models.keys()).transpose()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1714928729775,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"1S31FlpntIGz"},"outputs":[],"source":["# 3. Experiment with different models by changing different hyper-parameters\n","# (e.g, num_layers, hidden_size, optimiziers, activation_functions for RNNs, etc..) and evaluate the results for each of them on the testing set.\n","\n","# For this we will create a function that generalizes the code above, and we will change one parameter at a time in the following cells.\n","# IMPORTANT: this function assumes that the input are 8 HMMs at a time, it is important to put 8 as input for the plots to be displayed correctly.\n","\n","def evaluateModels(exp_hidden_size, exp_num_layers, exp_use_lstm, useSGD = False, lr=1e-3, epochs=1000):\n","    if len(exp_hidden_size) != 8:\n","        print(\"Input 8 different models.\")\n","        return\n","\n","    losses_models = {}\n","    test_accuracy_models = {}\n","    confusion_matrices = {}\n","    model_ids = []\n","\n","    print(\"------------ TRAINING ------------\\n\")\n","\n","    for hidden_size, num_layers, use_lstm in zip(exp_hidden_size, exp_num_layers, exp_use_lstm):\n","        model_id = f'H{hidden_size}_NL{num_layers}_LSTM{int(use_lstm)}'\n","        model_ids.append(model_id)\n","        print(f'Training: {model_id}')\n","\n","        seq_classifier = SequenceClassifier(use_lstm=use_lstm, num_layers=num_layers, hidden_size=hidden_size)\n","        seq_classifier.to(device)\n","\n","        if (useSGD == False):\n","            optimizer = torch.optim.Adam(seq_classifier.parameters(), lr=lr)\n","        else:\n","            optimizer = torch.optim.SGD(seq_classifier.parameters(), lr=lr)\n","\n","        losses_models[model_id] = train_multiclass_classifier(X_train_pt, Y_train_pt,\n","                                                            seq_classifier, optimizer,\n","                                                            loss_func, epochs=epochs)\n","\n","        test_accuracy_models[model_id], confusion_matrices[model_id] = test_multiclass_classifier(X_test_pt, Y_test_pt, seq_classifier)\n","\n","    print(\"\\n------------ CONFUSION MATRICES ------------\\n\")\n","\n","    #Plot confusion matrices\n","    f, axes = plt.subplots(2, 4, figsize=(12, 8), constrained_layout=True)\n","\n","    a = [axes[0, 0], axes[0, 1], axes[0, 2], axes[0, 3], axes[1, 0], axes[1, 1], axes[1, 2], axes[1, 3]]\n","    ctr = 0\n","    for model, cm in confusion_matrices.items():\n","        ConfusionMatrixDisplay(cm).plot(include_values=True, cmap=\"Blues\", ax=a[ctr], colorbar=False, values_format=\".0f\")\n","        a[ctr].set_title(model_ids[ctr], size=16)\n","        ctr += 1\n","\n","    plt.show()\n","\n","    print(\"\\n------------ LOSSES ------------\\n\")\n","\n","    # Plot losses' evolutions\n","    for loss_it in losses_models.values():\n","        plt.plot(loss_it)\n","    plt.legend(losses_models.keys())\n","    plt.xlabel(\"Iterations\")\n","    plt.ylabel(\"Loss (J)\")\n","    plt.show()\n","\n","    print(\"\\n------------ ACCURACIES ------------\\n\")\n","\n","    # Plot accuracies\n","    accuracy_df = pd.DataFrame(np.asarray(list(test_accuracy_models.values()))*100, columns=['accuracy %'],\n","                index=test_accuracy_models.keys()).transpose()\n","    print(accuracy_df.head())\n","\n","    accuracies = np.asarray(list(test_accuracy_models.values()))\n","    return accuracies, exp_hidden_size, exp_num_layers, exp_use_lstm"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1714928729775,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"gdZY56crtIGz"},"outputs":[],"source":["# Testing different values for exp_hidden_size\n","\n","exp_hidden_size = [1, 5, 10, 20] * 2\n","exp_num_layers = [1]*8\n","exp_use_lstm = [False]*4 + [True]*4\n","\n","acc, hd_sz, num_lay, use_lstm = evaluateModels(exp_hidden_size=exp_hidden_size, exp_num_layers=exp_num_layers, exp_use_lstm=exp_use_lstm)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":727},"executionInfo":{"elapsed":412,"status":"error","timestamp":1714928730177,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"jIpN6ftdtIG0","outputId":"81f93a0d-5145-41f7-b59a-860971a1a856"},"outputs":[],"source":["# Plot the evolution of accuracy with respect to hidden_size, for both lstm False and True.\n","f, axes = plt.subplots(1, 2, figsize=(15, 6))\n","axes[0].plot(hd_sz[:4], acc[:4])\n","axes[0].set_xlabel(\"Hidden size\")\n","axes[0].set_ylabel(\"Accuracy\")\n","axes[0].set_ylim([0, 1])\n","axes[0].set_title(\"use_lstm=False\")\n","axes[0].set_xticks(hd_sz[:4])\n","\n","axes[1].plot(hd_sz[-4:], acc[-4:])\n","axes[1].set_xlabel(\"Hidden size\")\n","axes[1].set_ylabel(\"Accuracy\")\n","axes[1].set_ylim([0, 1])\n","axes[1].set_title(\"use_lstm=True\")\n","axes[1].set_xticks(hd_sz[-4:])\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"executionInfo":{"elapsed":9,"status":"error","timestamp":1714928731954,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"7_ovzL-itIG0","outputId":"b7b5be51-64ec-4e31-9c49-876958987d0e"},"outputs":[],"source":["# Testing different values for exp_num_layers\n","\n","exp_hidden_size = [5]*8\n","exp_num_layers = [1,2,3,4]*2\n","exp_use_lstm = [False]*4 + [True]*4\n","\n","acc, hd_sz, num_lay, use_lstm = evaluateModels(exp_hidden_size=exp_hidden_size, exp_num_layers=exp_num_layers, exp_use_lstm=exp_use_lstm)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":727},"executionInfo":{"elapsed":635,"status":"error","timestamp":1714928732923,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"Mq-mhV5XtIG0","outputId":"b179e68c-ebed-4fbb-a661-686ba1038c81"},"outputs":[],"source":["# Plot the evolution of accuracy with respect to num_layers, for both lstm False and True.\n","f, axes = plt.subplots(1, 2, figsize=(15, 6))\n","axes[0].plot(num_lay[:4], acc[:4])\n","axes[0].set_xlabel(\"Num layers\")\n","axes[0].set_ylabel(\"Accuracy\")\n","axes[0].set_ylim([0, 1])\n","axes[0].set_title(\"use_lstm=False\")\n","axes[0].set_xticks(num_lay[:4])\n","\n","axes[1].plot(num_lay[-4:], acc[-4:])\n","axes[1].set_xlabel(\"Num layers\")\n","axes[1].set_ylabel(\"Accuracy\")\n","axes[1].set_ylim([0, 1])\n","axes[1].set_title(\"use_lstm=True\")\n","axes[1].set_xticks(num_lay[-4:])\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"executionInfo":{"elapsed":10,"status":"error","timestamp":1714928733358,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"vFEZ_lwFtIG1","outputId":"eee0151d-0bd7-496a-993a-be7ab63d9853"},"outputs":[],"source":["# Repeat experiment 1 with optimizer SGD\n","\n","exp_hidden_size = [1, 5, 10, 20] * 2\n","exp_num_layers = [1]*8\n","exp_use_lstm = [False]*4 + [True]*4\n","\n","acc, hd_sz, num_lay, use_lstm = evaluateModels(exp_hidden_size=exp_hidden_size, exp_num_layers=exp_num_layers, exp_use_lstm=exp_use_lstm, useSGD=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":727},"executionInfo":{"elapsed":7,"status":"error","timestamp":1714928734129,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"_oGeCTLmtIG1","outputId":"d0e7ed6b-08ff-4475-f5ad-016052151b6a"},"outputs":[],"source":["# Plot the evolution of accuracy with respect to hidden_size, for both lstm False and True.\n","f, axes = plt.subplots(1, 2, figsize=(15, 6))\n","axes[0].plot(hd_sz[:4], acc[:4])\n","axes[0].set_xlabel(\"Hidden size\")\n","axes[0].set_ylabel(\"Accuracy\")\n","axes[0].set_ylim([0, 1])\n","axes[0].set_title(\"use_lstm=False\")\n","axes[0].set_xticks(hd_sz[:4])\n","\n","axes[1].plot(hd_sz[-4:], acc[-4:])\n","axes[1].set_xlabel(\"Hidden size\")\n","axes[1].set_ylabel(\"Accuracy\")\n","axes[1].set_ylim([0, 1])\n","axes[1].set_title(\"use_lstm=True\")\n","axes[1].set_xticks(hd_sz[-4:])\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"executionInfo":{"elapsed":14648,"status":"error","timestamp":1714928749044,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"WJTgCv99tIG1","outputId":"f8a0a0e0-d6b5-4fb8-fe24-a274bd9c050c"},"outputs":[],"source":["# Final chosen model\n","\n","# Define parameters\n","exp_hidden_size = 20\n","exp_num_layers = 3\n","exp_use_lstm = True\n","lr = 7e-4\n","epochs = 10000\n","\n","print(\"------------ TRAINING ------------\\n\")\n","\n","model_id = f'H{exp_hidden_size}_NL{exp_num_layers}_LSTM{int(exp_use_lstm)}'\n","print(f'Training: {model_id}')\n","\n","seq_classifier = SequenceClassifier(use_lstm=exp_use_lstm, num_layers=exp_num_layers, hidden_size=exp_hidden_size)\n","seq_classifier.to(device)\n","\n","optimizer = torch.optim.Adam(seq_classifier.parameters(), lr=lr)\n","\n","loss_model = train_multiclass_classifier(X_train_pt, Y_train_pt,\n","                                                    seq_classifier, optimizer,\n","                                                    loss_func, epochs=epochs)\n","\n","test_accuracy_model, conf_matrix = test_multiclass_classifier(X_test_pt, Y_test_pt, seq_classifier)\n","\n","print(\"\\n------------ CONFUSION MATRICES ------------\\n\")\n","\n","#Plot confusion matrices\n","\n","ConfusionMatrixDisplay(conf_matrix).plot(include_values=True, cmap=\"Blues\", colorbar=False, values_format=\".0f\")\n","plt.title(model_id, size=16)\n","\n","plt.show()\n","\n","print(\"\\n------------ LOSSES ------------\\n\")\n","\n","# Plot losses' evolutions\n","plt.plot(loss_model)\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Loss (J)\")\n","plt.show()\n","\n","print(\"\\n------------ ACCURACIES ------------\\n\")\n","\n","# Plot accuracies\n","accuracy_df = pd.DataFrame(test_accuracy_model*100, columns=['accuracy %'],\n","            index=[model_id]).transpose()\n","print(accuracy_df.head())"]},{"cell_type":"markdown","metadata":{"id":"5NWk4UCNY6IU"},"source":["# Ex 2. Cryptoanalysis with corrupted messages using RNNs/LSTMs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1714928752719,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"uuW-po5hsQqt","outputId":"441fa621-7b2c-4aa9-dd40-ddaeeea1251c"},"outputs":[],"source":["'''\n","Similar to the exercise solved on P2-examples, you have to solve time-series modelling based on Cryptanalysis.\n","\n","For this exercise, you have training.pkl and testing_corrupted.pkl files.\n","These files contain a new encrypted dataset of 10000 sentences, 8000 for training and 2000 for the test.\n","Similar to the exercise solved in class, all the samples are encrypted using the Vigenère cipher with\n","7-length keyword. However, during the transmission of the test set, the encrypted data has been corrupted\n","with a new char (\"-\"), while the training set has been transmitted correctly.\n","\n","Each 32-length sentence contains 4 corrupted chars randomly distributed on the test set, representing 12,5 % of total data.\n","\n","Example:\n","\n","Received Ciphertext:   'WCPS-VH-JHIKCUSETJV-AW-LPVUAHFHH'\n","Correct Ciphertext (Not provided):   'WCPSEVHAJHIKCUSETJVKAWGLPVUAHFHH'\n","Ground truth:   'HEWENTALLOUTANDGAVEITHISBESTSHOT'\n","\n","Steps :\n","1. Discover the keyword used to encrypt the plaintext data. (hint: use Vigenère table and some samples)\n","2. Design/Implement an strategy to train the model so that it can be robust to missing characters in the testing samples ( to be discussed in class )\n","3. Train a sequential model to decode the corrupted test data.\n","4. Evaluate the decoding accuracy for the test set in terms of characters that were not corrupted or corrupted independently.\n","5. Try to improve the performance by applying some improvements over the model: stacked LSTMs,\n","hidden_sizes, embedding_size, optimizer, data augmentation during training, etc...\n","6. Visualize and discuss on the final results.\n","'''"]},{"cell_type":"markdown","metadata":{"id":"TF4mZU0xtIG2"},"source":["In 2. being robust means getting at least 87.5% accuracy (all non corrupt chars).\n","\n","For 3. we should have 2 losses, and 2 accuracies, 1 for normal chars, and 1 for corrupted chars. We should try to get 95% accuracy, 100% of the 87,5% correct chars and more or less 50% of corrupted chars are solved correctly.\n","\n","Advice: do data augmentation/transformation to try to make the training data look more like the testing data."]},{"cell_type":"markdown","metadata":{"id":"TUEWLxMusKiE"},"source":["# Sol 2."]},{"cell_type":"markdown","metadata":{"id":"0fb6Ded9tIG2"},"source":["## 1. Discover the keyword used to encrypt the plaintext data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4115,"status":"ok","timestamp":1714942481567,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"KpDXZqDia40H"},"outputs":[],"source":["# Load train and test files into memory\n","pkl_file = open(datadir+'training.pkl', 'rb')\n","train = pickle.load(pkl_file)\n","pkl_file.close()\n","\n","pkl_file = open(datadir+'testing_corrupted.pkl', 'rb')\n","test = pickle.load(pkl_file)\n","pkl_file.close()\n","\n","vocabulary = [char for char in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'] # Predefined vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":230,"status":"ok","timestamp":1714942483302,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"wCuSh_PftIG2"},"outputs":[],"source":["def int_to_vocabulary(message, vocabulary):\n","    decoded = ''\n","    for letter_code in message:\n","        decoded += vocabulary[letter_code]\n","    return decoded"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1714942484751,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"RHmNfWzDtIG3","outputId":"cff88bc0-0f63-40fa-a981-99cc54863a43"},"outputs":[],"source":["keyword = (train[0][0]-train[0][1]) % len(vocabulary)\n","print(f'Train Message encrypted:       {int_to_vocabulary(train[0][0],vocabulary)}')\n","print( 'The keyword for en/decryption: {kw}'.format(kw=int_to_vocabulary(keyword, vocabulary)))\n","print(f'Train Message decrypted:       {int_to_vocabulary(train[0][1],vocabulary)}')\n","print('\\nThe keyword (length {l}) is: {kw}'.format(l=7, kw=int_to_vocabulary(keyword, vocabulary)[:7]))"]},{"cell_type":"markdown","metadata":{"id":"KvqBVFPTtIG3"},"source":["## 2.Design/Implement an strategy to train the model"]},{"cell_type":"markdown","metadata":{"id":"UxViQSlAtIG3"},"source":["Proposed strategy: train as if there was another character in the vocabulary\n","\n","Problems of this strategy: this character won't be trained properly\n","\n","Solution to this problem: we could intentionally corrupt training data so the model learns how to interpret those appropiately"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1714942486620,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"4Q_y7e2-tIG3"},"outputs":[],"source":["corr_vocab = [char for char in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ-'] # Corrupted vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1714942487734,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"DQXfuKJstIG3"},"outputs":[],"source":["class CorruptedDataset(Dataset):\n","    # the data is in the form [img_name, boundaries]\n","    def __init__(self, data, p=0.125, corrupted_char=26):\n","        self.data = data.to(device)\n","        self.corr_prob = p\n","        self.corr_char = corrupted_char\n","        self.shape = data.shape\n","        self.batch_size = None\n","\n","        def corrupt_tensor(tensor):\n","            corrupted_idx = np.random.uniform(size=tensor.shape[0]) < self.corr_prob # Get random indexes for a sample with prob=corrupted_probability\n","            corr_item = tensor.clone()\n","            corr_item[corrupted_idx==True] = self.corr_char # Modify selected characters to the corrupted character\n","            return corr_item\n","        self.transform = corrupt_tensor\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.transform(self.data[idx])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1714942489330,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"V5WH2XD-tIG4"},"outputs":[],"source":["class CorruptedDecrypterNetwork(nn.Module):\n","    def __init__(self,\n","                 hidden_size : int = 8,\n","                 num_layers = 1,\n","                 num_letters = 26,\n","                 letters_embedding_size : int = 8,\n","                 use_lstm : bool = False):\n","        # Define RNN or LSTM architecture\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_letters = num_letters\n","        self.letters_embedder = torch.nn.Embedding(num_letters, letters_embedding_size)\n","        self.use_lstm = use_lstm\n","        self.softmax = nn.Softmax(dim=1)\n","        if(use_lstm):\n","            self.rnn = nn.LSTM(input_size = letters_embedding_size, hidden_size = hidden_size,\n","                               num_layers=num_layers, batch_first = True)\n","        else:\n","            self.rnn = nn.RNN(input_size = letters_embedding_size, hidden_size = hidden_size,\n","                              num_layers=num_layers, batch_first = True)\n","        self.last_linear = nn.Linear(hidden_size,num_letters)\n","\n","    def forward(self, X):\n","        N = X.shape[0]\n","        L = X.shape[1]\n","        embedded_letters = self.letters_embedder(X)\n","        # Get hidden states for all letters in the sequence\n","        hidden_states,_ = self.rnn(embedded_letters)\n","        # In case of multiple input sequneces flat (N,L,hidden_size) to (N*L,hidden_size) for linear layer\n","        hidden_states_concat = hidden_states.reshape(-1,self.hidden_size)\n","        # Get letters probability using the hidden states for each position in the sequence\n","        letters_loggits = self.last_linear(hidden_states_concat)\n","        # Use soft-max over logits and reshape to format (N,L,num_letteres)\n","        letters_probs = self.softmax(letters_loggits).reshape(N, L, self.num_letters)\n","        return letters_probs"]},{"cell_type":"markdown","metadata":{"id":"9ZRFCE31tIG4"},"source":["## 3-4. Training and testing"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1714942490430,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"aVLgzV6UtIG4"},"outputs":[],"source":["def train_test(model, num_epochs, loss_fn, optimizer,\n","            train_encrypted_loader, train_decrypted, test_encrypted, test_decrypted):\n","\n","    model = model.to(device)\n","    # train_encrypted_loader = train_encrypted_loader.data.to(device)\n","    train_decrypted = train_decrypted.to(device)\n","    test_encrypted = test_encrypted.to(device)\n","    test_decrypted = test_decrypted.to(device)\n","\n","    # Note that what were lists are now dictionaries to not return 12 elements\n","    train_loss_hist = { \"overall\" : [], \"corrupted\" : [], \"non-corrupted\" : [] }\n","    test_loss_hist = { \"overall\" : [], \"corrupted\" : [], \"non-corrupted\" : [] }\n","    train_acc_hist = { \"overall\" : [], \"corrupted\" : [], \"non-corrupted\" : [] }\n","    test_acc_hist = { \"overall\" : [], \"corrupted\" : [], \"non-corrupted\" : [] }\n","\n","    # Get corrupted indexes for the test dataset (always the same)\n","    test_corr_idxs = (test_encrypted == train_encrypted_loader.dataset.corr_char).nonzero(as_tuple=True)\n","    test_non_corr_idxs = (test_encrypted != train_encrypted_loader.dataset.corr_char).nonzero(as_tuple=True)\n","\n","    for epoch in range(num_epochs):\n","        # TRAINING AND BACK-PROPAGATION\n","        model.train()\n","        optimizer.zero_grad()\n","        batch = next(iter(train_encrypted_loader)) # Get whole augmented dataset (in a single batch)\n","        letters_probs = model(batch)\n","        train_loss = loss_fn(letters_probs.log().permute(0,2,1), # rearrange as to (N_sequences, N_letters, N_lenght_sequences)\n","                                        train_decrypted)\n","        train_loss.backward() # Backpropagate\n","        optimizer.step() # Update weights\n","\n","        # Get corrupted indexes for the batch (always changing)\n","        corr_idxs = (batch == train_encrypted_loader.dataset.corr_char).nonzero(as_tuple=True)\n","        non_corr_idxs = (batch != train_encrypted_loader.dataset.corr_char).nonzero(as_tuple=True)\n","\n","        # Split train loss into corrupted and non-corrupted for visualization\n","        train_loss_hist[\"overall\"].append(train_loss.item())\n","        train_loss_hist[\"corrupted\"].append(loss_fn(letters_probs[corr_idxs].log(), train_decrypted[corr_idxs]).item())\n","        train_loss_hist[\"non-corrupted\"].append(loss_fn(letters_probs[non_corr_idxs].log(), train_decrypted[non_corr_idxs]).item())\n","\n","        # Split train accuracy into corrupted and non-corrupted for visualization\n","        _, maxprob_letters_idx = letters_probs.max(dim=2) # get letter with maximum prob\n","        train_acc_hist[\"overall\"].append(((maxprob_letters_idx==train_decrypted)*1.0).mean().item())\n","        train_acc_hist[\"corrupted\"].append(((maxprob_letters_idx[corr_idxs]==train_decrypted[corr_idxs])*1.0).mean().item())\n","        train_acc_hist[\"non-corrupted\"].append(((maxprob_letters_idx[non_corr_idxs]==train_decrypted[non_corr_idxs])*1.0).mean().item())\n","\n","        # EVALUATION\n","        model.eval()\n","        letters_probs = model(test_encrypted)\n","        test_loss = loss_fn(letters_probs.log().permute(0,2,1), # rearrange as to (N_sequences, N_letters, N_lenght_sequences)\n","                                                test_decrypted)\n","\n","        # Split test loss into corrupted and non-corrupted for visualization\n","        test_loss_hist[\"overall\"].append(test_loss.item())\n","        test_loss_hist[\"corrupted\"].append(loss_fn(letters_probs[test_corr_idxs].log(), test_decrypted[test_corr_idxs]).item())\n","        test_loss_hist[\"non-corrupted\"].append(loss_fn(letters_probs[test_non_corr_idxs].log(), test_decrypted[test_non_corr_idxs]).item())\n","\n","        # Split test accuracy into corrupted and non-corrupted for visualization\n","        _, maxprob_letters_idx = letters_probs.max(dim=2) # get letter with maximum prob\n","        test_acc_hist[\"overall\"].append(((maxprob_letters_idx==test_decrypted)*1.0).mean().item())\n","        test_acc_hist[\"corrupted\"].append(((maxprob_letters_idx[test_corr_idxs]==test_decrypted[test_corr_idxs])*1.0).mean().item())\n","        test_acc_hist[\"non-corrupted\"].append(((maxprob_letters_idx[test_non_corr_idxs]==test_decrypted[test_non_corr_idxs])*1.0).mean().item())\n","\n","        if(epoch%50==0):\n","            print(f'Epoch {epoch} \\t Train Loss {round(train_loss.item(),3)} \\t Test Loss {round(test_loss.item(),3)} \\t Test Acc. (%)  {round(test_acc_hist[\"overall\"][-1]*100,1)}')\n","    print(f'Final Epoch \\t Train Loss {round(train_loss.item(),3)} \\t Test Loss {round(test_loss.item(),3)} \\t Test Acc. (%)  {round(test_acc_hist[\"overall\"][-1]*100,1)}')\n","    return model, train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2695,"status":"ok","timestamp":1714942493359,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"nBXsuJeXtIG4"},"outputs":[],"source":["train_encrypted = torch.concat([train_sample[0].unsqueeze(0) for train_sample in train],dim=0)\n","train_decrypted = torch.concat([train_sample[1].unsqueeze(0) for train_sample in train],dim=0)\n","test_encrypted = torch.concat([test_sample[0].unsqueeze(0) for test_sample in test],dim=0)\n","test_decrypted = torch.concat([test_sample[1].unsqueeze(0) for test_sample in test],dim=0)\n","\n","train_encrypted_dataset = CorruptedDataset(train_encrypted)\n","train_encrypted_loader = DataLoader(train_encrypted_dataset, batch_size=len(train_encrypted_dataset), shuffle=False)\n","\n","decrypter_network = CorruptedDecrypterNetwork(num_layers=1,\n","                                     num_letters=len(corr_vocab),\n","                                     hidden_size=16,\n","                                     use_lstm=True)\n","\n","num_epochs=1000\n","# weights [1/52, ..., 1/52, 1/2]\n","#          {   26 times   }\n","weights = [0.875/(len(corr_vocab)-1)]*(len(corr_vocab)-1) + [0.125]\n","weights = torch.Tensor(weights).to(device)\n","CE_loss = torch.nn.CrossEntropyLoss(weight=weights)\n","optimizer = torch.optim.Adam(decrypter_network.parameters(), lr=1e-2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":410806,"status":"ok","timestamp":1714929182945,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"AZEOWEh_tIHQ","outputId":"7d6a2398-56a6-4945-9535-ebdfc895f9ba"},"outputs":[],"source":["decrypter_network, train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist = train_test(decrypter_network,\n","                                                            num_epochs,\n","                                                            CE_loss,\n","                                                            optimizer,\n","                                                            train_encrypted_loader, train_decrypted, test_encrypted, test_decrypted)"]},{"cell_type":"markdown","metadata":{"id":"Fifv_yKgtIHR"},"source":["## 5. Visualizing results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1197,"status":"ok","timestamp":1714929207881,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"KqvgFvYOtIHR","outputId":"6dd0afc6-1ed9-4372-f70b-7cbdcb0cdb34"},"outputs":[],"source":["types = ['overall', 'corrupted', 'non-corrupted']\n","colors = {'overall': 'c', 'corrupted': 'm', 'non-corrupted': 'y'}\n","\n","def plot_loss_accuracy(train_loss, train_acc, test_loss, test_acc, title=None, types=types, colors=colors):\n","    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n","    fig.suptitle(title)\n","\n","    for i, set in enumerate(['train', 'test']):\n","        for j, metric in enumerate(['loss', 'accuracy']):\n","            for type in types:\n","\n","              ax = axes[j,i]\n","\n","              if metric=='loss':\n","                  if set=='train':\n","                      data = train_loss\n","                      label = 'Train Loss. '\n","                      subtitle = 'Train Loss '\n","                  else:\n","                      data = test_loss\n","                      label = 'Test Loss. '\n","                      subtitle = 'Test Loss '\n","                  ylabel =  'loss'\n","\n","              else:\n","                  if set=='train':\n","                      data = train_acc\n","                      label = 'Train Acc. '\n","                      subtitle = 'Train Accuracy '\n","                  else:\n","                      data = test_acc\n","                      label = 'Test Acc. '\n","                      subtitle = 'Test Accuracy '\n","                  ylabel = 'accuracy (%)'\n","\n","              ax.set_title(subtitle)\n","              ax.plot(data[type], '-'+colors[type], linewidth=1.0, label=label+type)\n","\n","              ax.set_xlabel('train step', fontsize=14)\n","              ax.set_ylabel(ylabel, fontsize=14)\n","              if metric == 'accuracy':\n","                  ax.set_ylim([0, 1])\n","              ax.set_xlim([0, num_epochs])\n","              ax.legend()\n","\n","\n","\n","    plt.tight_layout()\n","    plt.subplots_adjust(top=0.9)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_loss_accuracy(train_loss_hist, train_acc_hist, test_loss_hist, test_acc_hist)\n","\n","decrypter_network = decrypter_network.to(device)\n","for idx_sample in range(0,10):\n","    # Inference over single training sequence\n","    letters_probs = decrypter_network(test[idx_sample][0].unsqueeze(0).to(device))\n","    # get index of letter with max probability\n","    _,maxprob_letters_idx = letters_probs.max(dim=2)\n","    print('--------------------------------------')\n","    print(f'Original Message encrypted: {int_to_vocabulary(test[idx_sample][0],corr_vocab)}')\n","    print(f'Message decrypted: {int_to_vocabulary(test[idx_sample][1],vocabulary)}')\n","    print(f'Prediction Message decrypted: {int_to_vocabulary(maxprob_letters_idx[0],corr_vocab)}')\n","    acc = (1.0*(maxprob_letters_idx[0]==test[idx_sample][1].to(device))).mean().item()\n","    print(f'Prediction Message Accuracy : {round(acc,2)}')"]},{"cell_type":"markdown","metadata":{"id":"MVTeA381tIHR"},"source":["## 6. Improving the model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":268,"status":"ok","timestamp":1714929262749,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"Y2Xmhq6dtIHR"},"outputs":[],"source":["epochs = [500, 1000, 1500]\n","learning_rates = [1e-2, 1e-3, 5e-4]\n","use_lstm = [False, True]\n","num_layers = [1, 2]\n","hidden_sizes = [8, 16, 32, 48]\n","corruptions = [0, 0.125, 0.2, 0.4]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2432961,"status":"ok","timestamp":1714932186896,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"YbAW2jKDtIHS","outputId":"5b485228-36f6-45d1-e11c-6c8e280e0cca"},"outputs":[],"source":["for e in epochs:\n","    train_encrypted_dataset = CorruptedDataset(train_encrypted)\n","    train_encrypted_loader = DataLoader(train_encrypted_dataset, batch_size=len(train_encrypted_dataset), shuffle=False)\n","\n","    decrypter_network = CorruptedDecrypterNetwork(num_layers=1,\n","                                        num_letters=len(corr_vocab),\n","                                        hidden_size=16,\n","                                        use_lstm=True)\n","\n","    num_epochs=e\n","    optimizer = torch.optim.Adam(decrypter_network.parameters(), lr=1e-2)\n","\n","    decrypter_network, train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist = train_test(decrypter_network,\n","                                                                num_epochs,\n","                                                                CE_loss,\n","                                                                optimizer,\n","                                                                train_encrypted_loader, train_decrypted, test_encrypted, test_decrypted)\n","\n","    plot_loss_accuracy(train_loss_hist, train_acc_hist, test_loss_hist, test_acc_hist, title=str(e) + ' Epochs')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1609250,"status":"ok","timestamp":1714934616324,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"PMX6CpmUtIHS","outputId":"314444d8-0738-4001-f772-f8c989cb0ca6"},"outputs":[],"source":["for l in learning_rates:\n","    train_encrypted_dataset = CorruptedDataset(train_encrypted)\n","    train_encrypted_loader = DataLoader(train_encrypted_dataset, batch_size=len(train_encrypted_dataset), shuffle=False)\n","\n","    decrypter_network = CorruptedDecrypterNetwork(num_layers=1,\n","                                        num_letters=len(corr_vocab),\n","                                        hidden_size=16,\n","                                        use_lstm=True)\n","\n","    num_epochs=1000\n","    optimizer = torch.optim.Adam(decrypter_network.parameters(), lr=l)\n","\n","    decrypter_network, train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist = train_test(decrypter_network,\n","                                                                num_epochs,\n","                                                                CE_loss,\n","                                                                optimizer,\n","                                                                train_encrypted_loader, train_decrypted, test_encrypted, test_decrypted)\n","\n","    plot_loss_accuracy(train_loss_hist, train_acc_hist, test_loss_hist, test_acc_hist, title='Learning Rate is '+ str(l))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1628166,"status":"ok","timestamp":1714936292582,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"QNuComl6tIHS","outputId":"c8545da8-454e-4592-f4e0-a119d5b526c6"},"outputs":[],"source":["for r in use_lstm:\n","    if r == True:\n","        train_encrypted_dataset = CorruptedDataset(train_encrypted)\n","        train_encrypted_loader = DataLoader(train_encrypted_dataset, batch_size=len(train_encrypted_dataset), shuffle=False)\n","\n","        decrypter_network = CorruptedDecrypterNetwork(num_layers=1,\n","                                            num_letters=len(corr_vocab),\n","                                            hidden_size=16,\n","                                            use_lstm=r)\n","\n","        num_epochs=1000\n","        optimizer = torch.optim.Adam(decrypter_network.parameters(), lr=1e-2)\n","\n","        decrypter_network, train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist = train_test(decrypter_network,\n","                                                                    num_epochs,\n","                                                                    CE_loss,\n","                                                                    optimizer,\n","                                                                    train_encrypted_loader, train_decrypted, test_encrypted, test_decrypted)\n","\n","        plot_loss_accuracy(train_loss_hist, train_acc_hist, test_loss_hist, test_acc_hist, title='use_lstm='+ str(r))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1614225,"status":"ok","timestamp":1714937908902,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"OdNm1SUctIHS","outputId":"90825cde-c3bd-457f-c9fe-b4bd186f3504"},"outputs":[],"source":["for n in num_layers:\n","    train_encrypted_dataset = CorruptedDataset(train_encrypted)\n","    train_encrypted_loader = DataLoader(train_encrypted_dataset, batch_size=len(train_encrypted_dataset), shuffle=False)\n","\n","    decrypter_network = CorruptedDecrypterNetwork(num_layers=n,\n","                                        num_letters=len(corr_vocab),\n","                                        hidden_size=16,\n","                                        use_lstm=True)\n","\n","    num_epochs=1000\n","    optimizer = torch.optim.Adam(decrypter_network.parameters(), lr=1e-2)\n","\n","    decrypter_network, train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist = train_test(decrypter_network,\n","                                                                num_epochs,\n","                                                                CE_loss,\n","                                                                optimizer,\n","                                                                train_encrypted_loader, train_decrypted, test_encrypted, test_decrypted)\n","\n","    plot_loss_accuracy(train_loss_hist, train_acc_hist, test_loss_hist, test_acc_hist, title= str(n)+' Layers')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2417519,"status":"ok","timestamp":1714940417614,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"-8dIYe_MtIHT","outputId":"3d7b97fb-b439-458c-ae8e-35e7ec185fa0"},"outputs":[],"source":["for h in hidden_sizes:\n","    train_encrypted_dataset = CorruptedDataset(train_encrypted)\n","    train_encrypted_loader = DataLoader(train_encrypted_dataset, batch_size=len(train_encrypted_dataset), shuffle=False)\n","\n","    decrypter_network = CorruptedDecrypterNetwork(num_layers=2,\n","                                        num_letters=len(corr_vocab),\n","                                        hidden_size=h,\n","                                        use_lstm=True)\n","\n","    num_epochs=1000\n","    optimizer = torch.optim.Adam(decrypter_network.parameters(), lr=1e-2)\n","\n","    decrypter_network, train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist = train_test(decrypter_network,\n","                                                                num_epochs,\n","                                                                CE_loss,\n","                                                                optimizer,\n","                                                                train_encrypted_loader, train_decrypted, test_encrypted, test_decrypted)\n","\n","    plot_loss_accuracy(train_loss_hist, train_acc_hist, test_loss_hist, test_acc_hist, title='hidden_size='+str(h))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"JLEwbxlYtIHT","outputId":"ed78b0cb-ffa6-4f88-ffbe-7fb53f3bdd80"},"outputs":[],"source":["for c in corruptions:\n","    train_encrypted_dataset = CorruptedDataset(train_encrypted, p=c)\n","    train_encrypted_loader = DataLoader(train_encrypted_dataset, batch_size=len(train_encrypted_dataset), shuffle=False)\n","\n","    decrypter_network = CorruptedDecrypterNetwork(num_layers=1,\n","                                        num_letters=len(corr_vocab),\n","                                        hidden_size=16,\n","                                        use_lstm=True)\n","\n","    num_epochs=1000\n","    optimizer = torch.optim.Adam(decrypter_network.parameters(), lr=1e-2)\n","\n","    decrypter_network, train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist = train_test(decrypter_network,\n","                                                                num_epochs,\n","                                                                CE_loss,\n","                                                                optimizer,\n","                                                                train_encrypted_loader, train_decrypted, test_encrypted, test_decrypted)\n","\n","    plot_loss_accuracy(train_loss_hist, train_acc_hist, test_loss_hist, test_acc_hist, title='Corruption Probability is '+str(c))"]},{"cell_type":"markdown","metadata":{"id":"GN8gM4XpgAoR"},"source":["## 7. Final Model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":249,"status":"ok","timestamp":1714942503906,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"fvByO1o0gRtw"},"outputs":[],"source":["def_epochs = 1000\n","def_learning_rate = 1e-2\n","def_use_lstm = True\n","def_num_layer = 2\n","def_hidden_size = 128\n","def_corruption = 0.125"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":555},"executionInfo":{"elapsed":3143668,"status":"error","timestamp":1714945687609,"user":{"displayName":"JÚLIA OTHATS-DALÈS GIBERT","userId":"08518900160382405936"},"user_tz":-120},"id":"LsYdMJazf_Nv","outputId":"d427675d-ff43-4668-97c3-74331db58d32"},"outputs":[],"source":["train_encrypted_dataset = CorruptedDataset(train_encrypted, p=def_corruption)\n","train_encrypted_loader = DataLoader(train_encrypted_dataset, batch_size=len(train_encrypted_dataset), shuffle=False)\n","\n","decrypter_network = CorruptedDecrypterNetwork(num_layers=def_num_layer,\n","                                    num_letters=len(corr_vocab),\n","                                    hidden_size=def_hidden_size,\n","                                    use_lstm=def_use_lstm)\n","\n","num_epochs=def_epochs\n","optimizer = torch.optim.Adam(decrypter_network.parameters(), lr=def_learning_rate)\n","\n","decrypter_network, train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist = train_test(decrypter_network,\n","                                                            num_epochs,\n","                                                            CE_loss,\n","                                                            optimizer,\n","                                                            train_encrypted_loader, train_decrypted, test_encrypted, test_decrypted)\n","\n","plot_loss_accuracy(train_loss_hist, train_acc_hist, test_loss_hist, test_acc_hist, title='Final Model')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1ovEaKprm6pMDyF30u_UWpQQ86OgIFfNb","timestamp":1589917865212}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
