{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Practice 2"]},{"cell_type":"markdown","metadata":{},"source":["Student 1: Alejandro González Álvarez\n","\n","NIA 1: 252658\n","\n","Student 2: Luca Franceschi\n","\n","NIA 2: 253885\n","\n","Student 3: Júlia Othats-Dalès\n","\n","NIA 3: 254435"]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from torch import nn\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"]},{"cell_type":"markdown","metadata":{},"source":["# Google Drive (or not)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Comment if not in Google Colab\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# datadir = '/content/drive/My Drive/DeepLearning_2024/P2/MaterialP2/Data/'\n","datadir = 'Data/'"]},{"cell_type":"markdown","metadata":{},"source":["# GPU Acceleration (or not)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print if gpu acceleration is enabled\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","print(f\"Using {device} device\")"]},{"cell_type":"markdown","metadata":{"id":"G1WSF05s1023"},"source":["# Ex.1 - Experimenting with Multi-Class Sequence Classification using RNNs and LSTMs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1713964101238,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"5bAW1k6R1vgk","outputId":"59828d89-4571-4a0e-8e5a-81d0d863af1e"},"outputs":[],"source":["'''\n","Experimenting with Multi-Class Sequence Classification using RNNs and LSTMs\n","Steps :\n","1. Load and visualize the training and testing data in 'data/P2_E1.csv' as shown in the following code cell.\n","2. Following section 2 in the examples, train a RNN or LSTM to solve the multi-class sequence classification problem:\n","  -   Adapt the classification loss and the SequenceClassifier module\n","  -   Adapt the test_sequence_classifier function to compute the multi-class accuracy and be able to visualize the confusion matrix\n","3. Experiment with different models by changing different hyper-parameters (e.g, num_layers, hidden_size, optimiziers, activation_functions for RNNs, etc..) and evaluate  the results for each of them on the testing set.\n","4. Visualize analyse and discuss the results in the report.\n","'''"]},{"cell_type":"markdown","metadata":{"id":"cJoitFCf-9F1"},"source":["# Sol 1."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":28637,"status":"ok","timestamp":1713964129871,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"TEF-ZsLvYDMR","outputId":"1eb8cbd5-004e-47bc-fc2b-4bd94cd68e23"},"outputs":[],"source":["# 1. Load and visualize the training and testing data\n","data = np.load(datadir+'P2_E1.npz')\n","X_train, X_test = data['X_train'], data['X_test']\n","Y_train, Y_test = data['Y_train'], data['Y_test']\n","\n","print(f'Train Seqs: {X_train.shape}')\n","print(f'Train Labels: {Y_train.shape}')\n","print(f'Test Seqs: {X_test.shape}')\n","print(f'Test Labels: {Y_test.shape}')\n","\n","classes = np.unique(Y_train)\n","n_classes = len(classes)\n","colors = plt.cm.rainbow(np.linspace(0, 1, n_classes))\n","\n","fig, axs = plt.subplots(n_classes, 1, figsize=(10, 5*n_classes))\n","for class_idx in range(0, n_classes):\n","    axs[class_idx].plot(X_test[Y_test == class_idx, :].T, '-o', c=colors[class_idx])\n","    axs[class_idx].set_title(f'Tests Seqs. - Class {class_idx}')\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define module encapsulating a Sequence Classifier using RNN or LSTMs and setting different architecture hyper-parameters\n","\n","class SequenceClassifier(nn.Module):\n","    def __init__(self,\n","                 input_size : int = 1,\n","                 hidden_size : int = 5,\n","                 num_layers = 1,\n","                 num_classes : int = 6,\n","                 use_lstm : bool = False):\n","        # Define RNN or LSTM architecture\n","        super().__init__()\n","        self.use_lstm = use_lstm\n","        if(use_lstm):\n","            self.rnn = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n","                               num_layers=num_layers, batch_first = True)\n","        else:\n","            self.rnn = nn.RNN(input_size = input_size, hidden_size = hidden_size,\n","                              num_layers=num_layers, batch_first = True)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.last_linear = nn.Linear(hidden_size, num_classes)  # 6 is number of different classes\n","\n","    def forward(self, X):\n","        _, last_states = self.rnn(X)\n","        # Get last hidden state for last layer. Ignore cell state in case of LSTMs\n","        if(not self.use_lstm):\n","            last_hidden_state = last_states[-1,:,:].squeeze(0)\n","        else:\n","            last_hidden_state = last_states[0][-1,:,:].squeeze(0)\n","        # Get sequence label probability using the last hidden state\n","        output = self.softmax(self.last_linear(last_hidden_state))  # Using softmax instead of sigmoid\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the training model\n","def train_multiclass_classifier(X_train, Y_train, classifier, optimizer, loss_func, epochs=100):\n","    loss_epochs = []\n","    for epoch in range(epochs):\n","        optimizer.zero_grad()\n","        output = classifier(X_train)\n","        loss = loss_func(output, Y_train)\n","        loss_epochs.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","    print(f'Final loss: {loss.item()}')\n","    return np.asarray(loss_epochs)\n","\n","# Adapt the test_sequence_classifier function\n","def test_multiclass_classifier(X_test, Y_test, classifier):\n","    output = classifier(X_test)\n","    predicted_labels = torch.argmax(output, dim=1)\n","    correct_predictions = (predicted_labels == Y_test).sum().item()\n","    total_samples = len(Y_test)\n","    accuracy = correct_predictions / total_samples\n","    print(f'Test Accuracy: {accuracy:.4f}')\n","\n","    # Compute confusion matrix\n","    conf_matrix = confusion_matrix(Y_test.to(device), predicted_labels.to(device))\n","\n","    return accuracy, conf_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_pt = torch.from_numpy(X_train).float().unsqueeze(2).to(device)\n","Y_train_pt = torch.from_numpy(Y_train).long().to(device)\n","X_test_pt = torch.from_numpy(X_test).float().unsqueeze(2).to(device)\n","Y_test_pt = torch.from_numpy(Y_test).long().to(device)\n","\n","loss_func = nn.CrossEntropyLoss()\n","\n","exp_hidden_size = [1, 5]\n","exp_num_layers = [1, 2]\n","exp_use_lstm = [False, True]\n","\n","losses_models = {}\n","test_accuracy_models = {}\n","confusion_matrices = {}\n","\n","for hidden_size, num_layers, use_lstm in zip(exp_hidden_size, exp_num_layers, exp_use_lstm):\n","    model_id = f'H{hidden_size}_NL{num_layers}_LSTM{int(use_lstm)}'\n","    print(f'Training: {model_id}')\n","\n","    seq_classifier = SequenceClassifier(use_lstm=use_lstm, num_layers=num_layers, hidden_size=hidden_size)\n","    seq_classifier.to(device)\n","\n","    optimizer = torch.optim.Adam(seq_classifier.parameters(), lr=1e-3)\n","\n","    losses_models[model_id] = train_multiclass_classifier(X_train_pt, Y_train_pt,\n","                                                          seq_classifier, optimizer,\n","                                                          loss_func, epochs=1000)\n","\n","    test_accuracy_models[model_id], confusion_matrices[model_id] = test_multiclass_classifier(X_test_pt, Y_test_pt, seq_classifier)\n","\n","#Plot confusion matrices\n","f, axes = plt.subplots(1, 2, figsize=(20, 20))\n","\n","a = [axes[0], axes[1]]\n","ctr = 0\n","for model, cm in confusion_matrices.items():\n","    ConfusionMatrixDisplay(cm).plot(include_values=True, cmap=\"Blues\", ax=a[ctr], colorbar=False, values_format=\".0f\")\n","    a[ctr].set_title(\"Model \" + str(ctr), size=16)\n","    ctr += 1\n","\n","plt.show()\n","\n","for loss_it in losses_models.values():\n","    plt.plot(loss_it)\n","plt.legend(losses_models.keys())\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Loss (J)\")\n","plt.show()\n","\n","pd.DataFrame(np.asarray(list(test_accuracy_models.values()))*100, columns=['accuracy %'],\n","             index=test_accuracy_models.keys()).transpose()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 3. Experiment with different models by changing different hyper-parameters\n","# (e.g, num_layers, hidden_size, optimiziers, activation_functions for RNNs, etc..) and evaluate the results for each of them on the testing set.\n","\n","# For this we will create a function that generalizes the code above, and we will change one parameter at a time in the following cells.\n","# IMPORTANT: this function assumes that the input are 8 HMMs at a time, it is important to put 8 as input for the plots to be displayed correctly.\n","\n","def evaluateModels(exp_hidden_size, exp_num_layers, exp_use_lstm, useSGD = False, lr=1e-3, epochs=1000):\n","    if len(exp_hidden_size) != 8:\n","        print(\"Input 8 different models.\")\n","        return\n","    \n","    losses_models = {}\n","    test_accuracy_models = {}\n","    confusion_matrices = {}\n","    model_ids = []\n","\n","    print(\"------------ TRAINING ------------\\n\")\n","\n","    for hidden_size, num_layers, use_lstm in zip(exp_hidden_size, exp_num_layers, exp_use_lstm):\n","        model_id = f'H{hidden_size}_NL{num_layers}_LSTM{int(use_lstm)}'\n","        model_ids.append(model_id)\n","        print(f'Training: {model_id}')\n","\n","        seq_classifier = SequenceClassifier(use_lstm=use_lstm, num_layers=num_layers, hidden_size=hidden_size)\n","        seq_classifier.to(device)\n","\n","        if (useSGD == False):\n","            optimizer = torch.optim.Adam(seq_classifier.parameters(), lr=lr)\n","        else:\n","            optimizer = torch.optim.SGD(seq_classifier.parameters(), lr=lr)\n","\n","        losses_models[model_id] = train_multiclass_classifier(X_train_pt, Y_train_pt,\n","                                                            seq_classifier, optimizer,\n","                                                            loss_func, epochs=epochs)\n","\n","        test_accuracy_models[model_id], confusion_matrices[model_id] = test_multiclass_classifier(X_test_pt, Y_test_pt, seq_classifier)\n","\n","    print(\"\\n------------ CONFUSION MATRICES ------------\\n\")\n","\n","    #Plot confusion matrices\n","    f, axes = plt.subplots(2, 4, figsize=(12, 8), constrained_layout=True)\n","\n","    a = [axes[0, 0], axes[0, 1], axes[0, 2], axes[0, 3], axes[1, 0], axes[1, 1], axes[1, 2], axes[1, 3]]\n","    ctr = 0\n","    for model, cm in confusion_matrices.items():\n","        ConfusionMatrixDisplay(cm).plot(include_values=True, cmap=\"Blues\", ax=a[ctr], colorbar=False, values_format=\".0f\")\n","        a[ctr].set_title(model_ids[ctr], size=16)\n","        ctr += 1\n","\n","    plt.show()\n","\n","    print(\"\\n------------ LOSSES ------------\\n\")\n","\n","    # Plot losses' evolutions\n","    for loss_it in losses_models.values():\n","        plt.plot(loss_it)\n","    plt.legend(losses_models.keys())\n","    plt.xlabel(\"Iterations\")\n","    plt.ylabel(\"Loss (J)\")\n","    plt.show()\n","\n","    print(\"\\n------------ ACCURACIES ------------\\n\")\n","\n","    # Plot accuracies\n","    accuracy_df = pd.DataFrame(np.asarray(list(test_accuracy_models.values()))*100, columns=['accuracy %'],\n","                index=test_accuracy_models.keys()).transpose()\n","    print(accuracy_df.head())\n","\n","    accuracies = np.asarray(list(test_accuracy_models.values()))\n","    return accuracies, exp_hidden_size, exp_num_layers, exp_use_lstm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Testing different values for exp_hidden_size\n","\n","exp_hidden_size = [1, 5, 10, 20] * 2\n","exp_num_layers = [1]*8\n","exp_use_lstm = [False]*4 + [True]*4\n","\n","acc, hd_sz, num_lay, use_lstm = evaluateModels(exp_hidden_size=exp_hidden_size, exp_num_layers=exp_num_layers, exp_use_lstm=exp_use_lstm)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the evolution of accuracy with respect to hidden_size, for both lstm False and True.\n","f, axes = plt.subplots(1, 2, figsize=(15, 6))\n","axes[0].plot(hd_sz[:4], acc[:4])\n","axes[0].set_xlabel(\"Hidden size\")\n","axes[0].set_ylabel(\"Accuracy\")\n","axes[0].set_ylim([0, 1])\n","axes[0].set_title(\"use_lstm=False\")\n","axes[0].set_xticks(hd_sz[:4])\n","\n","axes[1].plot(hd_sz[-4:], acc[-4:])\n","axes[1].set_xlabel(\"Hidden size\")\n","axes[1].set_ylabel(\"Accuracy\")\n","axes[1].set_ylim([0, 1])\n","axes[1].set_title(\"use_lstm=True\")\n","axes[1].set_xticks(hd_sz[-4:])\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Testing different values for exp_num_layers\n","\n","exp_hidden_size = [5]*8\n","exp_num_layers = [1,2,3,4]*2\n","exp_use_lstm = [False]*4 + [True]*4\n","\n","acc, hd_sz, num_lay, use_lstm = evaluateModels(exp_hidden_size=exp_hidden_size, exp_num_layers=exp_num_layers, exp_use_lstm=exp_use_lstm)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the evolution of accuracy with respect to num_layers, for both lstm False and True.\n","f, axes = plt.subplots(1, 2, figsize=(15, 6))\n","axes[0].plot(num_lay[:4], acc[:4])\n","axes[0].set_xlabel(\"Num layers\")\n","axes[0].set_ylabel(\"Accuracy\")\n","axes[0].set_ylim([0, 1])\n","axes[0].set_title(\"use_lstm=False\")\n","axes[0].set_xticks(num_lay[:4])\n","\n","axes[1].plot(num_lay[-4:], acc[-4:])\n","axes[1].set_xlabel(\"Num layers\")\n","axes[1].set_ylabel(\"Accuracy\")\n","axes[1].set_ylim([0, 1])\n","axes[1].set_title(\"use_lstm=True\")\n","axes[1].set_xticks(num_lay[-4:])\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Repeat experiment 1 with optimizer SGD\n","\n","exp_hidden_size = [1, 5, 10, 20] * 2\n","exp_num_layers = [1]*8\n","exp_use_lstm = [False]*4 + [True]*4\n","\n","acc, hd_sz, num_lay, use_lstm = evaluateModels(exp_hidden_size=exp_hidden_size, exp_num_layers=exp_num_layers, exp_use_lstm=exp_use_lstm, useSGD=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the evolution of accuracy with respect to hidden_size, for both lstm False and True.\n","f, axes = plt.subplots(1, 2, figsize=(15, 6))\n","axes[0].plot(hd_sz[:4], acc[:4])\n","axes[0].set_xlabel(\"Hidden size\")\n","axes[0].set_ylabel(\"Accuracy\")\n","axes[0].set_ylim([0, 1])\n","axes[0].set_title(\"use_lstm=False\")\n","axes[0].set_xticks(hd_sz[:4])\n","\n","axes[1].plot(hd_sz[-4:], acc[-4:])\n","axes[1].set_xlabel(\"Hidden size\")\n","axes[1].set_ylabel(\"Accuracy\")\n","axes[1].set_ylim([0, 1])\n","axes[1].set_title(\"use_lstm=True\")\n","axes[1].set_xticks(hd_sz[-4:])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5NWk4UCNY6IU"},"source":["# Ex 2. Cryptoanalysis with corrupted messages using RNNs/LSTMs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713964129871,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"uuW-po5hsQqt","outputId":"79511c48-1aac-4770-8b48-1b5c510d2bb3"},"outputs":[],"source":["'''\n","Similar to the exercise solved on P2-examples, you have to solve time-series modelling based on Cryptanalysis.\n","\n","For this exercise, you have training.pkl and testing_corrupted.pkl files.\n","These files contain a new encrypted dataset of 10000 sentences, 8000 for training and 2000 for the test.\n","Similar to the exercise solved in class, all the samples are encrypted using the Vigenère cipher with\n","7-length keyword. However, during the transmission of the test set, the encrypted data has been corrupted\n","with a new char (\"-\"), while the training set has been transmitted correctly.\n","\n","Each 32-length sentence contains 4 corrupted chars randomly distributed on the test set, representing 12,5 % of total data.\n","\n","Example:\n","\n","Received Ciphertext:   'WCPS-VH-JHIKCUSETJV-AW-LPVUAHFHH'\n","Correct Ciphertext (Not provided):   'WCPSEVHAJHIKCUSETJVKAWGLPVUAHFHH'\n","Ground truth:   'HEWENTALLOUTANDGAVEITHISBESTSHOT'\n","\n","Steps :\n","1. Discover the keyword used to encrypt the plaintext data. (hint: use Vigenère table and some samples)\n","2. Design/Implement an strategy to train the model so that it can be robust to missing characters in the testing samples ( to be discussed in class )\n","3. Train a sequential model to decode the corrupted test data.\n","4. Evaluate the decoding accuracy for the test set in terms of characters that were not corrupted or corrupted independently.\n","5. Try to improve the performance by applying some improvements over the model: stacked LSTMs,\n","hidden_sizes, embedding_size, optimizer, data augmentation during training, etc...\n","6. Visualize and discuss on the final results.\n","'''"]},{"cell_type":"markdown","metadata":{},"source":["In 2. being robust means getting at least 87.5% accuracy (all non corrupt chars).\n","\n","For 3. we should have 2 losses, and 2 accuracies, 1 for normal chars, and 1 for corrupted chars. We should try to get 95% accuracy, 100% of the 87,5% correct chars and more or less 50% of corrupted chars are solved correctly.\n","\n","Advice: do data augmentation/transformation to try to make the training data look more like the testing data."]},{"cell_type":"markdown","metadata":{"id":"TUEWLxMusKiE"},"source":["# Sol 2."]},{"cell_type":"markdown","metadata":{},"source":["## 1. Discover the keyword used to encrypt the plaintext data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1713964140747,"user":{"displayName":"Adria Ruiz","userId":"10075760785236424538"},"user_tz":-120},"id":"KpDXZqDia40H"},"outputs":[],"source":["# Load train and test files into memory\n","pkl_file = open(datadir+'training.pkl', 'rb')\n","train = pickle.load(pkl_file)\n","pkl_file.close()\n","\n","pkl_file = open(datadir+'testing_corrupted.pkl', 'rb')\n","test = pickle.load(pkl_file)\n","pkl_file.close()\n","\n","vocabulary = [char for char in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'] # Predefined vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def int_to_vocabulary(message, vocabulary):\n","    decoded = ''\n","    for letter_code in message:\n","        decoded += vocabulary[letter_code]\n","    return decoded"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["keyword = (train[0][0]-train[0][1]) % len(vocabulary)\n","print(f'Train Message encrypted:       {int_to_vocabulary(train[0][0],vocabulary)}')\n","print( 'The keyword for en/decryption: {kw}'.format(kw=int_to_vocabulary(keyword, vocabulary)))\n","print(f'Train Message decrypted:       {int_to_vocabulary(train[0][1],vocabulary)}')\n","print('\\nThe keyword (length {l}) is: {kw}'.format(l=7, kw=int_to_vocabulary(keyword, vocabulary)[:7]))"]},{"cell_type":"markdown","metadata":{},"source":["## 2.Design/Implement an strategy to train the model"]},{"cell_type":"markdown","metadata":{},"source":["Proposed strategy: train as if there was another character in the vocabulary\n","\n","Problems of this strategy: this character won't be trained properly\n","\n","Solution to this problem: we could intentionally corrupt training data so the model learns how to interpret those appropiately"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["corr_vocab = [char for char in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ-'] # Corrupted vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CorruptedDataset(Dataset):\n","    # the data is in the form [img_name, boundaries]\n","    def __init__(self, data, p=0.125, corruption = True, corrupted_char=26):\n","        self.data = data.to(device)\n","        self.corr_prob = p\n","        self.corr_char = corrupted_char\n","        self.shape = data.shape\n","        self.batch_size = None\n","        self.corruption = corruption\n","\n","        def corrupt_tensor(tensor):\n","            corrupted_idx = np.random.uniform(size=tensor.shape[0]) < self.corr_prob\n","            corr_item = tensor.clone()\n","            corr_item[corrupted_idx==True] = self.corr_char\n","            return corr_item\n","        self.transform = corrupt_tensor\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        if (self.corruption):\n","            return self.transform(self.data[idx])\n","        return self.data[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DecrypterNetwork(nn.Module):\n","    def __init__(self,\n","                 hidden_size : int = 8,\n","                 num_layers = 1,\n","                 num_letters = 26,\n","                 letters_embedding_size : int = 8,\n","                 use_lstm : bool = False):\n","        # Define RNN or LSTM architecture\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_letters = num_letters\n","        self.letters_embedder = torch.nn.Embedding(num_letters, letters_embedding_size)\n","        self.use_lstm = use_lstm\n","        self.softmax = nn.Softmax(dim=1)\n","        if(use_lstm):\n","            self.rnn = nn.LSTM(input_size = letters_embedding_size, hidden_size = hidden_size,\n","                               num_layers=num_layers, batch_first = True)\n","        else:\n","            self.rnn = nn.RNN(input_size = letters_embedding_size, hidden_size = hidden_size,\n","                              num_layers=num_layers, batch_first = True)\n","        self.last_linear = nn.Linear(hidden_size,num_letters)\n","\n","    def forward(self, X):\n","        N = X.shape[0]\n","        L = X.shape[1]\n","        embedded_letters = self.letters_embedder(X)\n","        # Get hidden states for all letters in the sequence\n","        hidden_states,_ = self.rnn(embedded_letters)\n","        # In case of multiple input sequneces flat (N,L,hidden_size) to (N*L,hidden_size) for linear layer\n","        hidden_states_concat = hidden_states.reshape(-1,self.hidden_size)\n","        # Get letters probability using the hidden states for each position in the sequence\n","        letters_loggits = self.last_linear(hidden_states_concat)\n","        # Use soft-max over logits and reshape to format (N,L,num_letteres)\n","        letters_probs = self.softmax(letters_loggits).reshape(N, L, self.num_letters)\n","        return letters_probs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_test(model, num_epochs, loss_fn, optimizer,\n","            train_encrypted_loader, train_decrypted, test_encrypted, test_decrypted):\n","\n","    model = model.to(device)\n","    # train_encrypted_loader = train_encrypted_loader.data.to(device)\n","    train_decrypted = train_decrypted.to(device)\n","    test_encrypted = test_encrypted.to(device)\n","    test_decrypted = test_decrypted.to(device)\n","\n","    train_loss_hist = { \"overall\" : [], \"corrupted\" : [], \"non-corrupted\" : [] }\n","    test_loss_hist = { \"overall\" : [], \"corrupted\" : [], \"non-corrupted\" : [] }\n","    train_acc_hist = { \"overall\" : [], \"corrupted\" : [], \"non-corrupted\" : [] }\n","    test_acc_hist = { \"overall\" : [], \"corrupted\" : [], \"non-corrupted\" : [] }\n","\n","    for epoch in range(num_epochs):\n","        # TRAINING AND BACK-PROPAGATION\n","        model.train()\n","        optimizer.zero_grad()\n","        batch = next(iter(train_encrypted_loader))\n","        letters_probs = model(batch)\n","        train_loss = loss_fn(letters_probs.log().permute(0,2,1), # rearrange as to (N_sequences, N_letters, N_lenght_sequences)\n","                                        train_decrypted)\n","        train_loss.backward() # Backpropagate\n","        optimizer.step() # Update weights\n","        \n","        # Split train loss into corrupted and non-corrupted for visualization\n","        # Get corrupted indexes for the batch\n","        corr_idxs = (batch == train_encrypted_loader.dataset.corr_char).nonzero(as_tuple=True)\n","        non_corr_idxs = (batch != train_encrypted_loader.dataset.corr_char).nonzero(as_tuple=True)\n","\n","        train_loss_hist[\"overall\"].append(train_loss.item())\n","        train_loss_hist[\"corrupted\"].append(loss_fn(letters_probs[corr_idxs].log(), train_decrypted[corr_idxs]).item())\n","        train_loss_hist[\"non-corrupted\"].append(loss_fn(letters_probs[non_corr_idxs].log(), train_decrypted[non_corr_idxs]).item())\n","\n","        _, maxprob_letters_idx = letters_probs.max(dim=2) # get letter with maximum prob\n","        train_acc_hist[\"overall\"].append(((maxprob_letters_idx==train_decrypted)*1.0).mean().item())\n","        train_acc_hist[\"corrupted\"].append(((maxprob_letters_idx[corr_idxs]==train_decrypted[corr_idxs])*1.0).mean().item())\n","        train_acc_hist[\"non-corrupted\"].append(((maxprob_letters_idx[non_corr_idxs]==train_decrypted[non_corr_idxs])*1.0).mean().item())\n","\n","        # EVALUATION\n","        model.eval()\n","        letters_probs = model(test_encrypted)\n","        test_loss = loss_fn(letters_probs.log().permute(0,2,1), # rearrange as to (N_sequences, N_letters, N_lenght_sequences)\n","                                                test_decrypted)\n","        \n","        # Split test loss into corrupted and non-corrupted for visualization\n","        test_loss_hist[\"overall\"].append(test_loss.item())\n","        corr_idxs = (test_encrypted == train_encrypted_loader.dataset.corr_char).nonzero(as_tuple=True)\n","        test_loss_hist[\"corrupted\"].append(loss_fn(letters_probs[corr_idxs].log(), test_decrypted[corr_idxs]).item())\n","        non_corr_idxs = (test_encrypted != train_encrypted_loader.dataset.corr_char).nonzero(as_tuple=True)\n","        test_loss_hist[\"non-corrupted\"].append(loss_fn(letters_probs[non_corr_idxs].log(), test_decrypted[non_corr_idxs]).item())\n","        \n","        _, maxprob_letters_idx = letters_probs.max(dim=2) # get letter with maximum prob\n","        test_acc_hist[\"overall\"].append(((maxprob_letters_idx==test_decrypted)*1.0).mean().item())\n","        test_acc_hist[\"corrupted\"].append(((maxprob_letters_idx[corr_idxs]==test_decrypted[corr_idxs])*1.0).mean().item())\n","        test_acc_hist[\"non-corrupted\"].append(((maxprob_letters_idx[non_corr_idxs]==test_decrypted[non_corr_idxs])*1.0).mean().item())\n","\n","        if(epoch%50==0):\n","            print(f'Epoch {epoch} \\t Train Loss {round(train_loss.item(),3)} \\t Test Loss {round(test_loss.item(),3)} \\t Test Acc. (%)  {round(test_acc_hist['overall'][-1]*100,1)}')\n","    print(f'Final Epoch \\t Train Loss {round(train_loss.item(),3)} \\t Test Loss {round(test_loss.item(),3)} \\t Test Acc. (%)  {round(test_acc_hist['overall'][-1]*100,1)}')\n","    return model, train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_encrypted = torch.concat([train_sample[0].unsqueeze(0) for train_sample in train],dim=0)\n","train_decrypted = torch.concat([train_sample[1].unsqueeze(0) for train_sample in train],dim=0)\n","test_encrypted = torch.concat([test_sample[0].unsqueeze(0) for test_sample in test],dim=0)\n","test_decrypted = torch.concat([test_sample[1].unsqueeze(0) for test_sample in test],dim=0)\n","\n","train_encrypted_dataset = CorruptedDataset(train_encrypted)\n","train_encrypted_loader = DataLoader(train_encrypted_dataset, batch_size=len(train_encrypted_dataset), shuffle=False)\n","\n","decrypter_network = DecrypterNetwork(num_layers=1,\n","                                     num_letters=len(corr_vocab),\n","                                     hidden_size=16,\n","                                     use_lstm=True)\n","\n","num_epochs=50\n","CE_loss = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(decrypter_network.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["decrypter_network, train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist = train_test(decrypter_network,\n","                                                            num_epochs,\n","                                                            CE_loss,\n","                                                            optimizer,\n","                                                            train_encrypted_loader, train_decrypted, test_encrypted, test_decrypted)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["types = ['overall', 'corrupted', 'non-corrupted']\n","colors = {'overall': 'c', 'corrupted': 'm', 'non-corrupted': 'y'}\n","\n","for type in types:\n","    plt.plot(train_loss_hist[type], '-.'+colors[type], linewidth=1.0, label='Train loss '+type)\n","    plt.plot(test_loss_hist[type], '-'+colors[type], linewidth=1.0, label='Test loss '+type)\n","plt.xlabel('train step', fontsize=14)\n","plt.ylabel('loss', fontsize=14)\n","plt.legend()\n","plt.show()\n","\n","plt.plot(train_acc_hist[type], linewidth=3.0, label='test_acc')\n","plt.xlabel('train step', fontsize=14)\n","plt.ylabel('accuracy(%)', fontsize=14)\n","plt.ylim([0, 1])\n","plt.xlim([0, num_epochs])\n","plt.legend()\n","plt.show()\n","\n","decrypter_network = decrypter_network.to(device)\n","for idx_sample in range(0,10):\n","    # Inference over single training sequence\n","    letters_probs = decrypter_network(test[idx_sample][0].unsqueeze(0).to(device))\n","    # get index of letter with max probability\n","    _,maxprob_letters_idx = letters_probs.max(dim=2)\n","    print('--------------------------------------')\n","    print(f'Original Message encrypted: {int_to_vocabulary(test[idx_sample][0],corr_vocab)}')\n","    print(f'Message decrypted: {int_to_vocabulary(test[idx_sample][1],vocabulary)}')\n","    print(f'Prediction Message decrypted: {int_to_vocabulary(maxprob_letters_idx[0],corr_vocab)}')\n","    acc = (1.0*(maxprob_letters_idx[0]==test[idx_sample][1].to(device))).mean().item()\n","    print(f'Prediction Message Accuracy : {round(acc,2)}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1ovEaKprm6pMDyF30u_UWpQQ86OgIFfNb","timestamp":1589917865212}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
