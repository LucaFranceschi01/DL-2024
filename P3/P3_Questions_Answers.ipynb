{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9eOpEmm9KHG"
      },
      "source": [
        "# Practice 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rch0yy79KHL"
      },
      "source": [
        "Student 1: Alejandro González Álvarez\n",
        "\n",
        "NIA 1: 252658\n",
        "\n",
        "Student 2: Luca Franceschi\n",
        "\n",
        "NIA 2: 253885\n",
        "\n",
        "Student 3: Júlia Othats-Dalès\n",
        "\n",
        "NIA 3: 254435"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HlwJHylI9KHN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lukovsky/Documents/repos/DL-2024/.venv/lib64/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchinfo\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import scipy.io as sio\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pMcEe8i9KHP"
      },
      "source": [
        "# Google Drive (or not)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_itltoAu9KHP",
        "outputId": "594065d6-d477-4a48-e54c-dfd1dfff9ee6"
      },
      "outputs": [],
      "source": [
        "# Comment if not in Google Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# datadir = '/content/drive/My Drive/DeepLearning_2024/P3/Data/'\n",
        "datadir = 'Data/'\n",
        "\n",
        "# resultsdir = '/content/drive/My Drive/DeepLearning_2024/P3/Results/'\n",
        "resultsdir = 'Results/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeHaeaVi9KHQ"
      },
      "source": [
        "# GPU Acceleration (or not)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzA5X-q19KHQ",
        "outputId": "9117e802-afcb-49ed-9a12-d1ca382ee080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "# Print if gpu acceleration is enabled\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvWs2XIYNfhg"
      },
      "source": [
        "# **SVHN Dataset**\n",
        "In the following exercices, you will work with the SVHN dataset: http://ufldl.stanford.edu/housenumbers/\n",
        "\n",
        "It contains RGB images of street digits labeled from 1 to 10 similar to the MNIST dataset.\n",
        "\n",
        "The train and test sets are provided in the folder Data/svhn/ in .mat format.\n",
        "In the following we show an example of how to load the images and labels from these files. A DataLoader for this dataset is provided in the code examples for the lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "4JNFtVgFNaN3",
        "outputId": "3546e13d-9d41-4fa6-cf04-b1c06f705372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label: 1\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_mat = sio.loadmat(datadir + 'svhn/train_32x32.mat')\n",
        "\n",
        "images = train_mat['X']\n",
        "labels = train_mat['y'].squeeze()\n",
        "\n",
        "example_image, example_label = images[:,:,:,0], labels[0]\n",
        "\n",
        "print('Label: ' + str(example_label))\n",
        "plt.imshow(example_image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDulabPxYZXi"
      },
      "source": [
        "# Ex. 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NwccV9Wzl39"
      },
      "source": [
        "1. Try to obtain the maximum test accuracy possible in the SVHN dataset. For this purpose train/test different variants of the CNN provided in P3-Part1-Examples. You can explore different strategies:\n",
        "    - 1.1. Increase the size of the network by adding one ore more conv. layers. You can also increase the number of filters in each layer.\n",
        "    - 1.2. Try different optimizers such as Adam or SGD with momentum and modify the learning rate. You can check: https://pytorch.org/docs/stable/optim.html\n",
        "    - 1.3. Explore different random transformations during training ( Data augmentation ) such as random cropping with padding. You can check: https://pytorch.org/docs/stable/torchvision/transforms.html . Remember that these random transofrmation must not be used during testing.\n",
        "    - 1.4 Whatever you consider...\n",
        "\n",
        "2. Save all the different models , compare their test accuracy and analyse the results. Discuss what model has been shown more effective and why have you used the different strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA0aJf5Szxdv"
      },
      "source": [
        "# Sol. 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DHVvaY089KHW"
      },
      "outputs": [],
      "source": [
        "#Data Loader\n",
        "class SVHN(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, dataDir = datadir + 'svhn/train_32x32.mat', transform = None):\n",
        "        mat = sio.loadmat(dataDir)\n",
        "        self.data, self.labels = mat['X'], mat['y'].squeeze()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = self.data[:,:,:,index]\n",
        "        lbl = self.labels[index]\n",
        "\n",
        "        data = Image.fromarray(data)\n",
        "        # Apply a trasnformaiton to the image if it is indicated in the initalizer\n",
        "        if self.transform is not None :\n",
        "            data = self.transform(data)\n",
        "\n",
        "        # return the image and the label\n",
        "        return data,lbl\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "L_B3e0Tq9KHX"
      },
      "outputs": [],
      "source": [
        "def mean_std(dataDir= datadir + 'svhn/train_32x32.mat'):\n",
        "    mat = sio.loadmat(dataDir)\n",
        "    data = mat['X']\n",
        "    return data.mean(axis=(0,1,3)), data.std(axis=(0,1,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2usgyU39KHY",
        "outputId": "1234f332-7033-447c-ab23-0c928a41b80f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[111.60893668 113.16127466 120.56512767]\n"
          ]
        }
      ],
      "source": [
        "SHVN_mean, SHVN_std = mean_std()\n",
        "print(SHVN_mean)\n",
        "\n",
        "tr = tf.Compose([\n",
        "    tf.ToTensor(),\n",
        "    tf.Normalize(mean = SHVN_mean, std = SHVN_std)\n",
        "])\n",
        "\n",
        "# Define an standard CNN -> Two conv. blocks and linear layer\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5,  padding=2)\n",
        "        #input : 1 channel, output 16 channel, filter size : 5x5\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3,  padding=1)\n",
        "        #input : 1 channel, output 16 channel, filter size : 3x3\n",
        "\n",
        "        self.fc = nn.Linear(8*8*32, num_classes)\n",
        "\n",
        "        self.maxpool= nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.conv1(x)\n",
        "\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "\n",
        "        out = out.reshape(out.size(0), -1) #128,32,28,28 -> 128,28*28*32\n",
        "        out = self.fc(out) # we don't need sigmoid or other activation function at the end beacuse we will use nn.CrossEntropyLoss() (check documentation to understand why)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mUHyuMev9KHY"
      },
      "outputs": [],
      "source": [
        "CNN = ConvNet()\n",
        "CNN = CNN.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Q7lyDMv49KHY"
      },
      "outputs": [],
      "source": [
        "# Cross entropy loss for classification problems\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Initialize optimizer\n",
        "learning_rate = .001\n",
        "optimizer = torch.optim.Adam(CNN.parameters(),lr = learning_rate)\n",
        "\n",
        "num_epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load train and test datasets\n",
        "SVHNTrain = SVHN(datadir + 'svhn/train_32x32.mat', tr)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=SVHNTrain,\n",
        "                                            batch_size=256,\n",
        "                                            shuffle=True)\n",
        "\n",
        "SVHNTest = SVHN(datadir + 'svhn/test_32x32.mat',tr)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=SVHNTest,\n",
        "                                            batch_size=256,\n",
        "                                            shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uGOJWdC29KHZ"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "CNN.train() # Set the model in train mode\n",
        "total_step = len(train_loader)\n",
        "# Iterate over epochs\n",
        "for epoch in range(num_epochs):\n",
        "    # Iterate the dataset/batches\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Get batch of samples and labels\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = CNN(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                    .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(CNN.state_dict(), resultsdir+'model.ckpt')\n",
        "#to load : model.load_state_dict(torch.load(save_name_ori))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_mqZ6V39KHZ",
        "outputId": "9ac33a85-2383-4d10-db6d-718455b4292e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 12.5 %\n"
          ]
        }
      ],
      "source": [
        "#Load the model\n",
        "CNN.load_state_dict(torch.load(resultsdir+'model.ckpt'))\n",
        "\n",
        "# Test the model\n",
        "CNN.eval() # Set the model in evaluation mode\n",
        "\n",
        "# Compute testing accuracy\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # get network predictions\n",
        "        outputs = CNN(images)\n",
        "\n",
        "        # get predicted class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # compare with the ground-truth\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0qkdT02cLs2"
      },
      "source": [
        "### **Testing**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqJlILSeksxA"
      },
      "source": [
        "#### Number of convolution layers and number of filters (channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3fhZE9txcZu5"
      },
      "outputs": [],
      "source": [
        "#Increasing size of network (one more conv layer)\n",
        "\n",
        "class TestingConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "\n",
        "        super(TestingConvNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5,  padding=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3,  padding=1)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5,  padding=2) #layer added\n",
        "        self.fc = nn.Linear(4*4*128, num_classes) #fully connected layer modified\n",
        "\n",
        "        self.maxpool= nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NqTyOCkGkyUC"
      },
      "outputs": [],
      "source": [
        "CNN = TestingConvNet()\n",
        "CNN = CNN.to(device)\n",
        "\n",
        "# Cross entropy loss for classification problems\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Initialize optimizer\n",
        "learning_rate = .001\n",
        "optimizer = torch.optim.Adam(CNN.parameters(),lr = learning_rate)\n",
        "\n",
        "num_epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oi00N-rMnvhP"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "CNN.train() # Set the model in train mode\n",
        "total_step = len(train_loader)\n",
        "# Iterate over epochs\n",
        "for epoch in range(num_epochs):\n",
        "    # Iterate the dataset/batches\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Get batch of samples and labels\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = CNN(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                    .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(CNN.state_dict(), resultsdir+'model.ckpt')\n",
        "#to load : model.load_state_dict(torch.load(save_name_ori))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftdCgGLunx7P",
        "outputId": "f496474a-842e-48f2-d7f6-27ae2ad74809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 12.5 %\n"
          ]
        }
      ],
      "source": [
        "#Load the model\n",
        "CNN.load_state_dict(torch.load(resultsdir+'model.ckpt'))\n",
        "\n",
        "# Test the model\n",
        "CNN.eval() # Set the model in evaluation mode\n",
        "\n",
        "# Compute testing accuracy\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # get network predictions\n",
        "        outputs = CNN(images)\n",
        "\n",
        "        # get predicted class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # compare with the ground-truth\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTRZehWyuKSg"
      },
      "source": [
        "#### Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5M2jXfdOuJs3"
      },
      "outputs": [],
      "source": [
        "CNN = ConvNet()\n",
        "CNN = CNN.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizers = [(torch.optim.Adam(CNN.parameters(),lr = .001), \"Adam\"),\n",
        "              (torch.optim.SGD(CNN.parameters(), lr = .001, momentum = 0.9), \"SGD\")]\n",
        "\n",
        "num_epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtJ2lrnHv9eb",
        "outputId": "87c3d853-6fd7-44a1-bacc-5d13be277896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model with optimizer Adam: 12.5 %\n",
            "Test Accuracy of the model with optimizer SGD: 12.5 %\n"
          ]
        }
      ],
      "source": [
        "for optimizer, label in optimizers:\n",
        "\n",
        "    CNN.train() # Set the model in train mode\n",
        "    total_step = len(train_loader)\n",
        "    # Iterate over epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        # Iterate the dataset/batches\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            # Get batch of samples and labels\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = CNN(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                        .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # Save the model checkpoint\n",
        "    torch.save(CNN.state_dict(), resultsdir+'model.ckpt')\n",
        "    #to load : model.load_state_dict(torch.load(save_name_ori))\n",
        "\n",
        "    #Load the model\n",
        "    CNN.load_state_dict(torch.load(resultsdir+'model.ckpt'))\n",
        "\n",
        "    # Test the model\n",
        "    CNN.eval() # Set the model in evaluation mode\n",
        "\n",
        "    # Compute testing accuracy\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # get network predictions\n",
        "            outputs = CNN(images)\n",
        "\n",
        "            # get predicted class\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # compare with the ground-truth\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print('Test Accuracy of the model with optimizer {}: {} %'.format(label, 100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTz9Plh6xrlx"
      },
      "source": [
        "#### Learning rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "i01QLA60x3UW"
      },
      "outputs": [],
      "source": [
        "learning_rates = [.01, .05, .001, .005, .0001]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lwJhgTex-f-",
        "outputId": "832fe46e-e342-4cb3-f79a-dcd56f1a5f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of the model with learning rate 0.01: 12.5 %\n",
            "Test Accuracy of the model with learning rate 0.05: 12.5 %\n",
            "Test Accuracy of the model with learning rate 0.001: 12.5 %\n",
            "Test Accuracy of the model with learning rate 0.005: 12.5 %\n",
            "Test Accuracy of the model with learning rate 0.0001: 12.5 %\n"
          ]
        }
      ],
      "source": [
        "for lr in learning_rates:\n",
        "\n",
        "    optimizer = torch.optim.Adam(CNN.parameters(),lr = lr)\n",
        "\n",
        "    CNN.train() # Set the model in train mode\n",
        "    total_step = len(train_loader)\n",
        "    # Iterate over epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        # Iterate the dataset/batches\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            # Get batch of samples and labels\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = CNN(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                        .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # Save the model checkpoint\n",
        "    torch.save(CNN.state_dict(), resultsdir+'model.ckpt')\n",
        "    #to load : model.load_state_dict(torch.load(save_name_ori))\n",
        "\n",
        "    #Load the model\n",
        "    CNN.load_state_dict(torch.load(resultsdir+'model.ckpt'))\n",
        "\n",
        "    # Test the model\n",
        "    CNN.eval() # Set the model in evaluation mode\n",
        "\n",
        "    # Compute testing accuracy\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # get network predictions\n",
        "            outputs = CNN(images)\n",
        "\n",
        "            # get predicted class\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # compare with the ground-truth\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print('Test Accuracy of the model with learning rate {}: {} %'.format(lr, 100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7QTVmaKQg4X"
      },
      "source": [
        "# Ex. 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLVQ_LI6kSmK"
      },
      "source": [
        "## **Building your own efficient Convolutional Network architecture for SVHN**\n",
        "\n",
        "In the following,  you will need to build your own CNN architecture to predict digit numbers on the SVHN dataset. You are completely free to use any kind of layers and hyper-parameters for this purpose. Your goal is to acheive the maximum possible accuracy on the test set (the better, the higher score you'll get in the exercice). The only constraint is that your model should not contain more than 150K parameters. Below, we provide a simple code to compute the number of parameters in a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TFCESJFn1xZ"
      },
      "source": [
        "## Computing model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dnSiO4xQn7qU"
      },
      "outputs": [],
      "source": [
        "# Compute model paramters\n",
        "def compute_model_params(model):\n",
        "  params = 0\n",
        "  for p in model.parameters():\n",
        "    params+= p.numel()\n",
        "  return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qnJBuRZVjpOl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n1. Design and implement your own CNN. Given that the number of parameters must be small, you can check some papers focused on efficient CNN architectures and get some ideas.\\n  - MobileNet: https://arxiv.org/abs/1704.04861\\n  - MobileNet V2: https://arxiv.org/pdf/1801.04381.pdf\\n  - SqueezeNet: https://arxiv.org/abs/1602.07360\\n  - ShuffleNet: https://arxiv.org/abs/1707.01083\\n  - ESPNet V2: https://arxiv.org/abs/1811.11431\\n2. Train it and test it on SVHN using the provided code.\\n3. Discuss what approaches have you tried, why, and which ones have shown to be more beneficial.\\n'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "1. Design and implement your own CNN. Given that the number of parameters must be small, you can check some papers focused on efficient CNN architectures and get some ideas.\n",
        "  - MobileNet: https://arxiv.org/abs/1704.04861\n",
        "  - MobileNet V2: https://arxiv.org/pdf/1801.04381.pdf\n",
        "  - SqueezeNet: https://arxiv.org/abs/1602.07360\n",
        "  - ShuffleNet: https://arxiv.org/abs/1707.01083\n",
        "  - ESPNet V2: https://arxiv.org/abs/1811.11431\n",
        "2. Train it and test it on SVHN using the provided code.\n",
        "3. Discuss what approaches have you tried, why, and which ones have shown to be more beneficial.\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VObbYfbsQkls"
      },
      "source": [
        "# Sol. 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlKMweHXPO1z"
      },
      "source": [
        "## Define your own model and check the number of total parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://github.com/xin-w8023/SqueezeNet-PyTorch/blob/master/fire.py\n",
        "\n",
        "class Expand(torch.nn.Module):\n",
        "    def __init__(self, in_channels, e1_out_channles, e3_out_channles):\n",
        "        super(Expand, self).__init__()\n",
        "        self.conv_1x1 = torch.nn.Conv2d(in_channels, e1_out_channles, (1, 1))\n",
        "        self.conv_3x3 = torch.nn.Conv2d(in_channels, e3_out_channles, (3, 3), padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        o1 = self.conv_1x1(x)\n",
        "        o3 = self.conv_3x3(x)\n",
        "        return torch.cat((o1, o3), dim=1)\n",
        "\n",
        "\n",
        "class Fire(torch.nn.Module):\n",
        "    \"\"\"\n",
        "      Fire module in SqueezeNet\n",
        "      out_channles = e1x1 + e3x3\n",
        "      Eg.: input: ?xin_channelsx?x?\n",
        "           output: ?x(e1x1+e3x3)x?x?\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, s1x1, e1x1, e3x3):\n",
        "        super(Fire, self).__init__()\n",
        "\n",
        "        # squeeze \n",
        "        self.squeeze = torch.nn.Conv2d(in_channels, s1x1, (1, 1))\n",
        "        self.sq_act = torch.nn.LeakyReLU(0.1)\n",
        "\n",
        "        # expand\n",
        "        self.expand = Expand(s1x1, e1x1, e3x3)\n",
        "        self.ex_act = torch.nn.LeakyReLU(0.1)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sq_act(self.squeeze(x))\n",
        "        x = self.ex_act(self.expand(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "xMxY47BLpEPG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyModel Parameters: 49518\n"
          ]
        }
      ],
      "source": [
        "# ESPNet V2 Style Architecture\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "\n",
        "        super(MyModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fire1 = Fire(8, 4, 8, 8)\n",
        "        self.fire2 = Fire(16, 8, 8, 8)\n",
        "        self.fire3 = Fire(16, 8, 16, 16)\n",
        "        self.fire4 = Fire(32, 16, 32, 32)\n",
        "        self.fc = nn.Linear(64*8*8, num_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.fire1(out)\n",
        "        out = self.fire2(out)\n",
        "        out = self.maxpool(out)\n",
        "\n",
        "        out = self.fire3(out)\n",
        "        out = self.fire4(out)\n",
        "        out = self.maxpool(out)\n",
        "        \n",
        "        out = self.fc(out.view(out.shape[0], -1))\n",
        "        return out\n",
        "\n",
        "model = MyModel(num_classes=10)\n",
        "n_params = compute_model_params(model)\n",
        "print(\"MyModel Parameters: \" + str(n_params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "MyModel                                  [32, 10]                  --\n",
              "├─Conv2d: 1-1                            [32, 8, 32, 32]           224\n",
              "├─ReLU: 1-2                              [32, 8, 32, 32]           --\n",
              "├─Fire: 1-3                              [32, 16, 32, 32]          --\n",
              "│    └─Conv2d: 2-1                       [32, 4, 32, 32]           36\n",
              "│    └─LeakyReLU: 2-2                    [32, 4, 32, 32]           --\n",
              "│    └─Expand: 2-3                       [32, 16, 32, 32]          --\n",
              "│    │    └─Conv2d: 3-1                  [32, 8, 32, 32]           40\n",
              "│    │    └─Conv2d: 3-2                  [32, 8, 32, 32]           296\n",
              "│    └─LeakyReLU: 2-4                    [32, 16, 32, 32]          --\n",
              "├─Fire: 1-4                              [32, 16, 32, 32]          --\n",
              "│    └─Conv2d: 2-5                       [32, 8, 32, 32]           136\n",
              "│    └─LeakyReLU: 2-6                    [32, 8, 32, 32]           --\n",
              "│    └─Expand: 2-7                       [32, 16, 32, 32]          --\n",
              "│    │    └─Conv2d: 3-3                  [32, 8, 32, 32]           72\n",
              "│    │    └─Conv2d: 3-4                  [32, 8, 32, 32]           584\n",
              "│    └─LeakyReLU: 2-8                    [32, 16, 32, 32]          --\n",
              "├─MaxPool2d: 1-5                         [32, 16, 16, 16]          --\n",
              "├─Fire: 1-6                              [32, 32, 16, 16]          --\n",
              "│    └─Conv2d: 2-9                       [32, 8, 16, 16]           136\n",
              "│    └─LeakyReLU: 2-10                   [32, 8, 16, 16]           --\n",
              "│    └─Expand: 2-11                      [32, 32, 16, 16]          --\n",
              "│    │    └─Conv2d: 3-5                  [32, 16, 16, 16]          144\n",
              "│    │    └─Conv2d: 3-6                  [32, 16, 16, 16]          1,168\n",
              "│    └─LeakyReLU: 2-12                   [32, 32, 16, 16]          --\n",
              "├─Fire: 1-7                              [32, 64, 16, 16]          --\n",
              "│    └─Conv2d: 2-13                      [32, 16, 16, 16]          528\n",
              "│    └─LeakyReLU: 2-14                   [32, 16, 16, 16]          --\n",
              "│    └─Expand: 2-15                      [32, 64, 16, 16]          --\n",
              "│    │    └─Conv2d: 3-7                  [32, 32, 16, 16]          544\n",
              "│    │    └─Conv2d: 3-8                  [32, 32, 16, 16]          4,640\n",
              "│    └─LeakyReLU: 2-16                   [32, 64, 16, 16]          --\n",
              "├─MaxPool2d: 1-8                         [32, 64, 8, 8]            --\n",
              "├─Linear: 1-9                            [32, 10]                  40,970\n",
              "==========================================================================================\n",
              "Total params: 49,518\n",
              "Trainable params: 49,518\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 105.45\n",
              "==========================================================================================\n",
              "Input size (MB): 0.39\n",
              "Forward/backward pass size (MB): 21.50\n",
              "Params size (MB): 0.20\n",
              "Estimated Total Size (MB): 22.09\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torchinfo.summary(model, (32, 3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dangurPPcgm"
      },
      "source": [
        "## Train your model on SVHN\n",
        "It is not allowed to change training hyper-parameters such as learning rate, batch size or number of epochs. You can only modify the architecture definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "ccmWoqr5u9Xj"
      },
      "outputs": [],
      "source": [
        "# Train function\n",
        "def train(CNN, train_loader, optimizer, num_epochs=5, model_name='model.ckpt', device='cpu'):\n",
        "    CNN.train() # Set the model in train mode\n",
        "    total_step = len(train_loader)\n",
        "    losses_list = []\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Iterate over epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        # Iterate the dataset\n",
        "        loss_avg = 0\n",
        "        nBatches = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            # Get batch of samples and labels\n",
        "            images = images.to(device)\n",
        "            labels = labels.type(torch.LongTensor).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = CNN(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_avg += loss.to(device).item()\n",
        "            nBatches+=1\n",
        "            if (i+1) % 100 == 0:\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                       .format(epoch+1, num_epochs, i+1, total_step, loss_avg / nBatches))\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                       .format(epoch+1, num_epochs, i+1, total_step, loss_avg / nBatches))\n",
        "        losses_list.append(loss_avg / nBatches)\n",
        "        torch.save(CNN.state_dict(), resultsdir+ '/' + model_name)\n",
        "\n",
        "    return losses_list\n",
        "\n",
        "# Test funcion\n",
        "def test(CNN, test_loader):\n",
        "  with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # get network predictions\n",
        "            outputs = CNN(images)\n",
        "\n",
        "            # get predicted class\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # compare with the ground-truth\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        # return accuracy\n",
        "        return 100 * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "9vib9IY2vAkP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/200], Step [1/1], Loss: 2.3049\n",
            "Epoch [2/200], Step [1/1], Loss: 2.2957\n",
            "Epoch [3/200], Step [1/1], Loss: 2.2791\n",
            "Epoch [4/200], Step [1/1], Loss: 2.2569\n",
            "Epoch [5/200], Step [1/1], Loss: 2.2314\n",
            "Epoch [6/200], Step [1/1], Loss: 2.2045\n",
            "Epoch [7/200], Step [1/1], Loss: 2.1781\n",
            "Epoch [8/200], Step [1/1], Loss: 2.1538\n",
            "Epoch [9/200], Step [1/1], Loss: 2.1328\n",
            "Epoch [10/200], Step [1/1], Loss: 2.1161\n",
            "Epoch [11/200], Step [1/1], Loss: 2.1042\n",
            "Epoch [12/200], Step [1/1], Loss: 2.0970\n",
            "Epoch [13/200], Step [1/1], Loss: 2.0938\n",
            "Epoch [14/200], Step [1/1], Loss: 2.0934\n",
            "Epoch [15/200], Step [1/1], Loss: 2.0942\n",
            "Epoch [16/200], Step [1/1], Loss: 2.0950\n",
            "Epoch [17/200], Step [1/1], Loss: 2.0947\n",
            "Epoch [18/200], Step [1/1], Loss: 2.0930\n",
            "Epoch [19/200], Step [1/1], Loss: 2.0900\n",
            "Epoch [20/200], Step [1/1], Loss: 2.0863\n",
            "Epoch [21/200], Step [1/1], Loss: 2.0824\n",
            "Epoch [22/200], Step [1/1], Loss: 2.0787\n",
            "Epoch [23/200], Step [1/1], Loss: 2.0756\n",
            "Epoch [24/200], Step [1/1], Loss: 2.0733\n",
            "Epoch [25/200], Step [1/1], Loss: 2.0717\n",
            "Epoch [26/200], Step [1/1], Loss: 2.0707\n",
            "Epoch [27/200], Step [1/1], Loss: 2.0702\n",
            "Epoch [28/200], Step [1/1], Loss: 2.0699\n",
            "Epoch [29/200], Step [1/1], Loss: 2.0698\n",
            "Epoch [30/200], Step [1/1], Loss: 2.0696\n",
            "Epoch [31/200], Step [1/1], Loss: 2.0694\n",
            "Epoch [32/200], Step [1/1], Loss: 2.0690\n",
            "Epoch [33/200], Step [1/1], Loss: 2.0685\n",
            "Epoch [34/200], Step [1/1], Loss: 2.0678\n",
            "Epoch [35/200], Step [1/1], Loss: 2.0671\n",
            "Epoch [36/200], Step [1/1], Loss: 2.0664\n",
            "Epoch [37/200], Step [1/1], Loss: 2.0657\n",
            "Epoch [38/200], Step [1/1], Loss: 2.0651\n",
            "Epoch [39/200], Step [1/1], Loss: 2.0646\n",
            "Epoch [40/200], Step [1/1], Loss: 2.0641\n",
            "Epoch [41/200], Step [1/1], Loss: 2.0638\n",
            "Epoch [42/200], Step [1/1], Loss: 2.0635\n",
            "Epoch [43/200], Step [1/1], Loss: 2.0632\n",
            "Epoch [44/200], Step [1/1], Loss: 2.0630\n",
            "Epoch [45/200], Step [1/1], Loss: 2.0629\n",
            "Epoch [46/200], Step [1/1], Loss: 2.0627\n",
            "Epoch [47/200], Step [1/1], Loss: 2.0625\n",
            "Epoch [48/200], Step [1/1], Loss: 2.0623\n",
            "Epoch [49/200], Step [1/1], Loss: 2.0622\n",
            "Epoch [50/200], Step [1/1], Loss: 2.0620\n",
            "Epoch [51/200], Step [1/1], Loss: 2.0619\n",
            "Epoch [52/200], Step [1/1], Loss: 2.0617\n",
            "Epoch [53/200], Step [1/1], Loss: 2.0616\n",
            "Epoch [54/200], Step [1/1], Loss: 2.0614\n",
            "Epoch [55/200], Step [1/1], Loss: 2.0612\n",
            "Epoch [56/200], Step [1/1], Loss: 2.0611\n",
            "Epoch [57/200], Step [1/1], Loss: 2.0609\n",
            "Epoch [58/200], Step [1/1], Loss: 2.0608\n",
            "Epoch [59/200], Step [1/1], Loss: 2.0607\n",
            "Epoch [60/200], Step [1/1], Loss: 2.0606\n",
            "Epoch [61/200], Step [1/1], Loss: 2.0605\n",
            "Epoch [62/200], Step [1/1], Loss: 2.0604\n",
            "Epoch [63/200], Step [1/1], Loss: 2.0603\n",
            "Epoch [64/200], Step [1/1], Loss: 2.0602\n",
            "Epoch [65/200], Step [1/1], Loss: 2.0602\n",
            "Epoch [66/200], Step [1/1], Loss: 2.0601\n",
            "Epoch [67/200], Step [1/1], Loss: 2.0600\n",
            "Epoch [68/200], Step [1/1], Loss: 2.0600\n",
            "Epoch [69/200], Step [1/1], Loss: 2.0599\n",
            "Epoch [70/200], Step [1/1], Loss: 2.0599\n",
            "Epoch [71/200], Step [1/1], Loss: 2.0598\n",
            "Epoch [72/200], Step [1/1], Loss: 2.0597\n",
            "Epoch [73/200], Step [1/1], Loss: 2.0597\n",
            "Epoch [74/200], Step [1/1], Loss: 2.0596\n",
            "Epoch [75/200], Step [1/1], Loss: 2.0596\n",
            "Epoch [76/200], Step [1/1], Loss: 2.0595\n",
            "Epoch [77/200], Step [1/1], Loss: 2.0595\n",
            "Epoch [78/200], Step [1/1], Loss: 2.0594\n",
            "Epoch [79/200], Step [1/1], Loss: 2.0594\n",
            "Epoch [80/200], Step [1/1], Loss: 2.0593\n",
            "Epoch [81/200], Step [1/1], Loss: 2.0593\n",
            "Epoch [82/200], Step [1/1], Loss: 2.0592\n",
            "Epoch [83/200], Step [1/1], Loss: 2.0592\n",
            "Epoch [84/200], Step [1/1], Loss: 2.0592\n",
            "Epoch [85/200], Step [1/1], Loss: 2.0591\n",
            "Epoch [86/200], Step [1/1], Loss: 2.0591\n",
            "Epoch [87/200], Step [1/1], Loss: 2.0591\n",
            "Epoch [88/200], Step [1/1], Loss: 2.0590\n",
            "Epoch [89/200], Step [1/1], Loss: 2.0590\n",
            "Epoch [90/200], Step [1/1], Loss: 2.0590\n",
            "Epoch [91/200], Step [1/1], Loss: 2.0589\n",
            "Epoch [92/200], Step [1/1], Loss: 2.0589\n",
            "Epoch [93/200], Step [1/1], Loss: 2.0589\n",
            "Epoch [94/200], Step [1/1], Loss: 2.0588\n",
            "Epoch [95/200], Step [1/1], Loss: 2.0588\n",
            "Epoch [96/200], Step [1/1], Loss: 2.0588\n",
            "Epoch [97/200], Step [1/1], Loss: 2.0588\n",
            "Epoch [98/200], Step [1/1], Loss: 2.0587\n",
            "Epoch [99/200], Step [1/1], Loss: 2.0587\n",
            "Epoch [100/200], Step [1/1], Loss: 2.0587\n",
            "Epoch [101/200], Step [1/1], Loss: 2.0587\n",
            "Epoch [102/200], Step [1/1], Loss: 2.0586\n",
            "Epoch [103/200], Step [1/1], Loss: 2.0586\n",
            "Epoch [104/200], Step [1/1], Loss: 2.0586\n",
            "Epoch [105/200], Step [1/1], Loss: 2.0586\n",
            "Epoch [106/200], Step [1/1], Loss: 2.0585\n",
            "Epoch [107/200], Step [1/1], Loss: 2.0585\n",
            "Epoch [108/200], Step [1/1], Loss: 2.0585\n",
            "Epoch [109/200], Step [1/1], Loss: 2.0585\n",
            "Epoch [110/200], Step [1/1], Loss: 2.0585\n",
            "Epoch [111/200], Step [1/1], Loss: 2.0585\n",
            "Epoch [112/200], Step [1/1], Loss: 2.0584\n",
            "Epoch [113/200], Step [1/1], Loss: 2.0584\n",
            "Epoch [114/200], Step [1/1], Loss: 2.0584\n",
            "Epoch [115/200], Step [1/1], Loss: 2.0584\n",
            "Epoch [116/200], Step [1/1], Loss: 2.0584\n",
            "Epoch [117/200], Step [1/1], Loss: 2.0583\n",
            "Epoch [118/200], Step [1/1], Loss: 2.0583\n",
            "Epoch [119/200], Step [1/1], Loss: 2.0583\n",
            "Epoch [120/200], Step [1/1], Loss: 2.0583\n",
            "Epoch [121/200], Step [1/1], Loss: 2.0583\n",
            "Epoch [122/200], Step [1/1], Loss: 2.0583\n",
            "Epoch [123/200], Step [1/1], Loss: 2.0583\n",
            "Epoch [124/200], Step [1/1], Loss: 2.0582\n",
            "Epoch [125/200], Step [1/1], Loss: 2.0582\n",
            "Epoch [126/200], Step [1/1], Loss: 2.0582\n",
            "Epoch [127/200], Step [1/1], Loss: 2.0582\n",
            "Epoch [128/200], Step [1/1], Loss: 2.0582\n",
            "Epoch [129/200], Step [1/1], Loss: 2.0582\n",
            "Epoch [130/200], Step [1/1], Loss: 2.0582\n",
            "Epoch [131/200], Step [1/1], Loss: 2.0582\n",
            "Epoch [132/200], Step [1/1], Loss: 2.0581\n",
            "Epoch [133/200], Step [1/1], Loss: 2.0581\n",
            "Epoch [134/200], Step [1/1], Loss: 2.0581\n",
            "Epoch [135/200], Step [1/1], Loss: 2.0581\n",
            "Epoch [136/200], Step [1/1], Loss: 2.0581\n",
            "Epoch [137/200], Step [1/1], Loss: 2.0581\n",
            "Epoch [138/200], Step [1/1], Loss: 2.0581\n",
            "Epoch [139/200], Step [1/1], Loss: 2.0581\n",
            "Epoch [140/200], Step [1/1], Loss: 2.0581\n",
            "Epoch [141/200], Step [1/1], Loss: 2.0580\n",
            "Epoch [142/200], Step [1/1], Loss: 2.0580\n",
            "Epoch [143/200], Step [1/1], Loss: 2.0580\n",
            "Epoch [144/200], Step [1/1], Loss: 2.0580\n",
            "Epoch [145/200], Step [1/1], Loss: 2.0580\n",
            "Epoch [146/200], Step [1/1], Loss: 2.0580\n",
            "Epoch [147/200], Step [1/1], Loss: 2.0580\n",
            "Epoch [148/200], Step [1/1], Loss: 2.0580\n",
            "Epoch [149/200], Step [1/1], Loss: 2.0580\n",
            "Epoch [150/200], Step [1/1], Loss: 2.0580\n",
            "Epoch [151/200], Step [1/1], Loss: 2.0579\n",
            "Epoch [152/200], Step [1/1], Loss: 2.0579\n",
            "Epoch [153/200], Step [1/1], Loss: 2.0579\n",
            "Epoch [154/200], Step [1/1], Loss: 2.0579\n",
            "Epoch [155/200], Step [1/1], Loss: 2.0579\n",
            "Epoch [156/200], Step [1/1], Loss: 2.0579\n",
            "Epoch [157/200], Step [1/1], Loss: 2.0579\n",
            "Epoch [158/200], Step [1/1], Loss: 2.0579\n",
            "Epoch [159/200], Step [1/1], Loss: 2.0579\n",
            "Epoch [160/200], Step [1/1], Loss: 2.0579\n",
            "Epoch [161/200], Step [1/1], Loss: 2.0579\n",
            "Epoch [162/200], Step [1/1], Loss: 2.0579\n",
            "Epoch [163/200], Step [1/1], Loss: 2.0579\n",
            "Epoch [164/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [165/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [166/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [167/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [168/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [169/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [170/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [171/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [172/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [173/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [174/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [175/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [176/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [177/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [178/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [179/200], Step [1/1], Loss: 2.0578\n",
            "Epoch [180/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [181/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [182/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [183/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [184/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [185/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [186/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [187/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [188/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [189/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [190/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [191/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [192/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [193/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [194/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [195/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [196/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [197/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [198/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [199/200], Step [1/1], Loss: 2.0577\n",
            "Epoch [200/200], Step [1/1], Loss: 2.0576\n"
          ]
        }
      ],
      "source": [
        "#Train MyModel\n",
        "my_model = MyModel()\n",
        "\n",
        "#Initialize optimizer\n",
        "learning_rate = .01\n",
        "optimizer = torch.optim.SGD(my_model.parameters(),lr = learning_rate,\n",
        "                            weight_decay=1e-5, momentum=0.9)\n",
        "\n",
        "my_model = my_model.to(device)\n",
        "\n",
        "losses_list = train(my_model, train_loader, optimizer, num_epochs=200, model_name='my_net.ckpt', device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4ilm-FoPmLX"
      },
      "source": [
        "## Test your model\n",
        "As a reference, 93% of accuracy can be easily achieved by using only ~55000 parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "LHT86IPSzOYi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy MyNet: 12.5\n"
          ]
        }
      ],
      "source": [
        "# Show results for all the trained models\n",
        "SVHNTest = SVHN(datadir+'/svhn/test_32x32.mat',tr)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=SVHNTest,\n",
        "                                               batch_size=256,\n",
        "                                               shuffle=True)\n",
        "\n",
        "#\n",
        "my_net = MyModel()\n",
        "my_net.load_state_dict(torch.load(resultsdir + 'my_net.ckpt'))\n",
        "my_net.to(device)\n",
        "acc = test(my_net, test_loader)\n",
        "print('Accuracy MyNet: ' + str(acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7UMjGA9TOUM"
      },
      "source": [
        "# Ex. 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HXvqONt8TNuc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n1. Use the implemented architecture in the previous exercice to solve the transfer learning\\n   task provided in the examples.\\n2. Try to fine-tune not only the last layer for the CNN but a larger subset of parameters.\\n2. Report the test accuracy in each case and discuss the results.\\n'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "1. Use the implemented architecture in the previous exercice to solve the transfer learning\n",
        "   task provided in the examples.\n",
        "2. Try to fine-tune not only the last layer for the CNN but a larger subset of parameters.\n",
        "2. Report the test accuracy in each case and discuss the results.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o31sdIuJUHex"
      },
      "source": [
        "## Sol. 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSGN1MHVIQBJ"
      },
      "source": [
        "### Initialize DataLoaders for Transfer Learning experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "lftsNbnEHeoh"
      },
      "outputs": [],
      "source": [
        "# Big dataset with numbers from 1 to 8\n",
        "SVHNTrain_TL = SVHN(datadir+'/svhn_tl/tl_train_32x32.mat',tr)\n",
        "tl_train_loader = torch.utils.data.DataLoader(dataset=SVHNTrain_TL,\n",
        "                                               batch_size=256,\n",
        "                                               shuffle=True)\n",
        "\n",
        "# 200 samples of 0s and 9s\n",
        "SVHNTrain_TL_0_9 = SVHN(datadir+'/svhn_tl/tl_train_9_10_32x32.mat',tr)\n",
        "tl_train_loader_0_9 = torch.utils.data.DataLoader(dataset=SVHNTrain_TL_0_9,\n",
        "                                                  batch_size=64,\n",
        "                                                  shuffle=True)\n",
        "# Test dataset with 0 and 9s\n",
        "SVHNTest_TL_0_9 = SVHN(datadir+'/svhn_tl/tl_test_9_10_32x32.mat',tr)\n",
        "tl_test_loader_0_9 = torch.utils.data.DataLoader(dataset=SVHNTest_TL_0_9,\n",
        "                                                  batch_size=64,\n",
        "                                                  shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXbLlY4aC4Ga"
      },
      "source": [
        "### Pre-train your model with the big dataset with numbers from 1 to 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "o2zitIY5tJRg"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "Target 8 is out of bounds.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m resnet_tl \u001b[38;5;241m=\u001b[39m resnet_tl\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m losses_it \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet_tl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtl_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtl_mynet_svhn.ckpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[26], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(CNN, train_loader, optimizer, num_epochs, model_name, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m CNN(images)\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/Documents/repos/DL-2024/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/repos/DL-2024/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/repos/DL-2024/.venv/lib64/python3.12/site-packages/torch/nn/modules/loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/repos/DL-2024/.venv/lib64/python3.12/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 8 is out of bounds."
          ]
        }
      ],
      "source": [
        "#Train ResNet\n",
        "resnet_tl = MyModel(num_classes=8)\n",
        "#Initialize optimizer\n",
        "learning_rate = .1\n",
        "optimizer = torch.optim.SGD(resnet_tl.parameters(),lr = learning_rate, weight_decay=1e-5, momentum=0.9)\n",
        "\n",
        "resnet_tl = resnet_tl.to(device)\n",
        "\n",
        "losses_it = train(resnet_tl, tl_train_loader, optimizer, num_epochs=10, model_name='tl_mynet_svhn.ckpt', device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0HrjNVIFNKN"
      },
      "source": [
        "### Fine-tune the pretrained network with the small dataset of 9s and 0s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDemFpsnsw4J"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
