{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Practice 1"]},{"cell_type":"markdown","metadata":{},"source":["Student 1: <font color=\"blue\">Alejandro González Álvarez</font>\n","\n","NIA 1: <font color=\"blue\">252658</font>\n","\n","Student 2: <font color=\"blue\">Luca Franceschi</font>\n","\n","NIA 2: <font color=\"blue\">253885</font>\n","\n","Student 3: <font color=\"blue\">Júlia Othats-Dalès</font>\n","\n","NIA 3: <font color=\"blue\">254435</font>"]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib.colors import ListedColormap\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import torch"]},{"cell_type":"markdown","metadata":{},"source":["# Useful Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hyp_lr = 0.05\n","hyp_epochs = 10000"]},{"cell_type":"markdown","metadata":{"id":"fXH0lVHuisCw"},"source":["# Ex. 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYqQjgWlivdx"},"outputs":[],"source":["'''\n","Steps :\n","  1. Load train.csv and test.csv containing a 2D dataset (features X and Y) with two different classes\n","  2. Visualize the dataset and normalize the dimensions of the samples to have zero mean and unit standard deviation\n","  3. Implement an MLP using Numpy in order to solve a classification problem trying to estimate the classes of the samples\n","  4. Train the MLP and visualize the decision boundary in 2D of the classification. Plot the training loss for each iteration.\n","  5. Use the learned MLP to estimate the classes of the data in test.csv. Compute the accuracy in training and testing dataset.\n","'''"]},{"cell_type":"markdown","metadata":{"id":"L3Chuxpjiv3W"},"source":["## Sol. 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XhQU8t66wYJ9"},"outputs":[],"source":["#1. Load train.csv and test.csv\n","train_df = pd.read_csv('Data/train.csv')\n","test_df = pd.read_csv('Data/test.csv')\n","\n","# Show 5 first elements of the datasets\n","print(\"Training dataset:\\n\")\n","print(train_df.head(5))\n","print(\"Testing dataset:\\n\")\n","print(test_df.head(5))\n","\n","#2. Visualize both training and testing data\n","train_labels = train_df[['C']].values\n","test_labels = test_df[['C']].values\n","\n","fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n","fig.suptitle('Not normalized')\n","axes[0].set_box_aspect(1)\n","axes[1].set_box_aspect(1)\n","axes[0].scatter(train_df.X, train_df.Y, c = train_labels)\n","axes[0].set_title(\"Training dataset\")\n","axes[1].scatter(test_df.X, test_df.Y, c = test_labels)\n","axes[1].set_title(\"Testing dataset\")\n","plt.show()\n","\n","# Normalize data\n","train_x = np.array(train_df[[\"X\"]].values)\n","train_y = np.array(train_df[[\"Y\"]].values)\n","test_x = np.array(test_df[[\"X\"]].values)\n","test_y = np.array(test_df[[\"Y\"]].values)\n","\n","train_data = np.column_stack((train_x, train_y))\n","test_data = np.column_stack((test_x, test_y))\n","\n","normalized_train = (train_data - np.mean(train_data, axis=0)) / np.std(train_data, axis=0)\n","normalized_test = (test_data - np.mean(train_data, axis=0)) / np.std(train_data, axis=0)\n","\n","# normalized_train = (train_df[[\"X\", \"Y\"]] - np.mean(train_df[[\"X\", \"Y\"]], axis = 0)) / np.std(train_df[[\"X\", \"Y\"]], axis = 0)\n","# normalized_test = (test_df[[\"X\", \"Y\"]] - np.mean(test_df[[\"X\", \"Y\"]], axis = 0)) / np.std(test_df[[\"X\", \"Y\"]], axis = 0)\n","\n","fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n","fig.suptitle('Normalized')\n","axes[0].set_box_aspect(1)\n","axes[1].set_box_aspect(1)\n","axes[0].scatter(normalized_train[:,0], normalized_train[:,1], c = train_labels)\n","axes[0].set_title(\"Training dataset\")\n","axes[1].scatter(normalized_test[:,0], normalized_test[:,1], c = test_labels)\n","axes[1].set_title(\"Testing dataset\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#3. Implementation of the MLP Class:\n","class MLP(object):\n","    def __init__(self,inputNode=2, hiddenNode=3, outputNode=1):\n","        #Define MLP hyper-parameters\n","        self.inputLayerSize = inputNode # number of input dimensions (x)\n","        self.outputLayerSize = outputNode # number of output dimensions (y)\n","        self.hiddenLayerSize = hiddenNode # Dimension of intermediate layer (W_2)\n","\n","        #Weights (parameters)\n","        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n","        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n","\n","    def forward(self, X):\n","        #Compute inputs from outputs\n","        self.z2 = np.dot(X, self.W1)\n","        self.a2 = self.sigmoid(self.z2)\n","        self.z3 = np.dot(self.a2, self.W2)\n","        yHat = self.sigmoid(self.z3)\n","        return yHat\n","\n","    def sigmoid(self, z):\n","        #Apply sigmoid activation function to scalar, vector, or matrix\n","        return 1/(1+np.exp(-z))\n","\n","    # Derivative of sigmoid and loss with respect their inputs\n","    def sigmoidPrime(self, z):\n","        #Gradient of sigmoid\n","        return np.exp(-z)/((1+np.exp(-z))**2)    # sigmoid(z) * (1 - sigmoid(z))\n","\n","    def loss(self, yHat, y):\n","        #Compute loss for given X, y, use weights already stored in class.\n","        J = -np.mean(y*np.log(yHat) + (1-y)*np.log(1-yHat))\n","        return J\n","\n","    # Derivative of parameters\n","    def backward(self, X, yHat, y):\n","        self.yHat = yHat\n","        delta3 = (yHat - y)     # dL / dz3\n","        dJdW2 = np.dot(self.a2.T, delta3)\n","\n","        delta2 = np.dot(delta3, self.W2.T) * self.sigmoidPrime(self.z2)\n","        dJdW1 = np.dot(X.T, delta2)\n","        return dJdW1,dJdW2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to train a neural network (NN)\n","def train(NN:MLP, X, y, epoch=10000, lr=.1):\n","    list_loss = []\n","\n","    for i in range(epoch):\n","        #Compute derivative with respect to W and W2 for a given X and y:\n","        yHat = NN.forward(X)\n","\n","        gradW1,gradW2 = NN.backward(X, yHat, y)\n","        #now update the weight using gradient descent\n","        NN.W1 -= gradW1 * lr\n","        NN.W2 -= gradW2 * lr\n","\n","        if i%10 == 0 :\n","            loss = NN.loss(yHat, y)\n","            if i%100 == 0:\n","                print('Loss {}={}'.format(i, loss))\n","            list_loss.append(loss)\n","\n","    return list_loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict(NN, X):\n","    return np.round(NN.forward(X)).astype(int)\n","\n","def accuracy(y, yHat):\n","    error = 0\n","    error = np.sum(np.logical_xor(y, yHat))\n","    return 1 - (error/len(y))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Instantiate an MLP NN\n","mlp = MLP()\n","\n","#4. Train the MLP, visualize decision boundary in 2D, and plot loss at each iteration\n","list_loss = train(mlp, train_df[[\"X\", \"Y\"]], train_labels, epoch=hyp_epochs, lr=hyp_lr)\n","\n","# arange used for better visualization of iterations\n","plt.plot(10 * np.arange(len(list_loss)), list_loss)\n","plt.title('MLP lr=.1')\n","plt.xlabel('Iterations')\n","plt.ylabel('Loss (J) value')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize the decision boundary in 2D of the classification\n","def visualize_boundaries(NN, range_x, range_y,\n","                         norm_train=None, norm_test=None, labels_train=None, labels_test=None,\n","                         cmap=ListedColormap(['#7E1E9C99', '#FFFF1499']),\n","                         title='MLP'):\n","    \n","    xx, yy = np.meshgrid(np.arange(range_x[0], range_x[1], 0.01), np.arange(range_y[0], range_y[1], 0.01))\n","    input = np.column_stack((xx.flatten(), yy.flatten()))\n","\n","    pred = predict(NN, input)\n","    pred = pred.reshape(xx.shape)\n","\n","    fig, ax = plt.subplots(1, 2, sharey=True, figsize=(8,4.5))\n","\n","    ax[0].set_box_aspect(1)\n","    ax[1].set_box_aspect(1)\n","\n","    fig.suptitle('Decision Boundaries ({})'.format(title))\n","    ax[0].set_title('Training dataset')\n","    ax[1].set_title('Test dataset')\n","    ax[0].set_ylabel('Y direction')\n","    ax[0].set_xlabel('X direction')\n","    ax[1].set_xlabel('X direction')\n","\n","    ax[0].pcolormesh(xx, yy, pred, cmap=cmap)\n","    ax[1].pcolormesh(xx, yy, pred, cmap=cmap)\n","\n","    if norm_train.all() != None and norm_test.all() != None and labels_train.all() != None and labels_test.all() != None:\n","        ax[0].scatter(norm_train[:,0], norm_train[:,1], c=labels_train)\n","        ax[1].scatter(norm_test[:,0], norm_test[:,1], c=labels_test)\n","\n","    plt.tight_layout()\n","    plt.show()\n","    # plt.contour([normalized_test[:, 0], normalized_test[:, 1]], )\n","\n","visualize_boundaries(mlp, (-2.5, 2.5), (-2.5, 2.5), normalized_train, normalized_test, train_labels, test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 5. Apply on test dataset, and accuracy\n","test_yHat = predict(mlp, normalized_test)\n","train_yHat = predict(mlp, normalized_train)\n","\n","train_accuracy = accuracy(train_labels, train_yHat)\n","test_accuracy = accuracy(test_labels, test_yHat)\n","\n","print(\"Accuracy obtained in training dataset: %.2f\" % (train_accuracy * 100) + \" %.\")\n","print(\"Accuracy obtained in test dataset: %.2f\" % (test_accuracy * 100) + \" %.\")"]},{"cell_type":"markdown","metadata":{"id":"hZB8-00E6-NF"},"source":["# Ex. 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KcXbgAb47FI8"},"outputs":[],"source":["'''\n","Instead of using the implemented optimizer with standard Gradient Descent, in this exercice you will use SGD with momentum.\n","This optimizer tipically accelerates the convergence of the optimization and, thus, it reduce the number of iterations during gradient-descent.\n","Steps :\n","  1. Try to understand how momentum is implemented reading carefully https://www.ruder.io/optimizing-gradient-descent/#momentum\n","  2. Modifying OptimSGD in the examples, implement SGD with momentum in a new class call OptimMom.\n","  3. Train the MLP implmented in numpy with this new optimizer.\n","  4. Use different values for the momentum paramter [0,1] and plot the evolution of the training loss during gradient-descent. Compare the results using the different values and standard gradient descent.\n","'''\n"]},{"cell_type":"markdown","metadata":{"id":"F9aMFlbO7Fj4"},"source":["## Sol. 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKMkugDvwXgQ"},"outputs":[],"source":["class OptimMom(object):\n","    def __init__(self,learning_rate = .001, gamma = 0.9):\n","        self.lr = learning_rate\n","        self.gamma = gamma\n","        self.last = 0\n","    # receive the parameters of the MLP and the computed gradients and update the latter\n","    def step(self,weight_list,gradient):\n","        uw = []\n","        for w,grd in zip(weight_list,gradient):\n","            VdW = (1-self.gamma)*self.last + self.gamma*grd\n","            uw.append(w - self.lr * VdW)\n","        return uw"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Same training procedure than befor but using the optimizer class\n","def train_woptimizer(NN,X, y, epoch = 5000, optim = None):\n","    list_loss = []\n","\n","    for i in range(epoch):\n","        #Compute derivative with respect to W and W2 for a given X and y:\n","        yHat = NN.forward(X)\n","\n","        gradW1,gradW2 = NN.backward(X,yHat,y)\n","        '''#now update the weight using gradient descent and the optimizer class\n","        NN.W1 -= gradW1 * lr\n","        NN.W2 -= gradW2 * lr'''\n","        NN.W1, NN.W2 = optim.step([NN.W1,NN.W2],[gradW1,gradW2])\n","\n","        if i%10 == 0 :\n","            loss = NN.loss(yHat, y)\n","            if i%100 == 0:\n","                print('Loss {}={}'.format(i, loss))\n","            list_loss.append(loss)\n","\n","    return list_loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optim = OptimMom(learning_rate=hyp_lr, gamma=0.9)\n","mlpMom = MLP()\n","\n","list_loss = train_woptimizer(mlpMom, train_df[[\"X\", \"Y\"]], np.array([train_labels]).reshape(100, 1), epoch=hyp_epochs, optim=optim)\n","\n","# arange used for better visualization of iterations\n","plt.plot(10 * np.arange(len(list_loss)), list_loss)\n","plt.title('MLP Momentum lr=.1')\n","plt.xlabel('Iterations')\n","plt.ylabel('Loss (J) value')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["visualize_boundaries(mlpMom, (-2.5, 2.5), (-2.5, 2.5), normalized_train, normalized_test, train_labels, test_labels, title='MLP Momentum')"]},{"cell_type":"markdown","metadata":{"id":"CiIne9w6PkRr"},"source":["# Ex. 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V03WbzcvRzDl"},"outputs":[],"source":["'''\n","Steps :\n","  1. Do the same as Ex. 1 but implementing the MLP using PyTorch.\n","  2. Train different vesions of the MLP by varying the number of hidden neurons and learing rates\n","  3. Analyse the results in each case and discuss them.\n","\n","HINT: Check in https://pytorch.org/docs/stable/nn.html what loss function implemented in PyTorch you have to use in this case.\n","'''"]},{"cell_type":"markdown","metadata":{"id":"qr0ElP9vRzvQ"},"source":["## Sol. 3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print if gpu acceleration is enabled\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","print(f\"Using {device} device\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yzUBzYUvwbW6"},"outputs":[],"source":["class MLP_PyTorch(nn.Module):\n","    def __init__(self, inputNode=2, hiddenNode=3, outputNode=1):\n","        super(MLP_PyTorch, self).__init__()\n","        # Define Hyperparameters\n","        self.inputLayerSize = inputNode\n","        self.outputLayerSize = outputNode\n","        self.hiddenLayerSize = hiddenNode\n","\n","        self.Linear1 = nn.Linear(self.inputLayerSize, self.hiddenLayerSize)\n","        self.Linear2 = nn.Linear(self.hiddenLayerSize, self.outputLayerSize)\n","\n","        nn.init.normal_(self.Linear1.weight, mean=0, std=0.1)\n","        nn.init.normal_(self.Linear2.weight, mean=0, std=0.1)\n","\n","        self.sigmoid = nn.Sigmoid() # Module implementing the sigmoid funciton\n","        self.loss = nn.BCELoss() # Not MSE as in the expample\n","\n","    def forward(self, X):\n","        out = self.Linear1(X) # First Linear Layer\n","        out = self.sigmoid(out) # activation function\n","        out = self.Linear2(out) # Second Linear Layer\n","        y_hat = self.sigmoid(out) # final activation function\n","        return y_hat\n","\n","def train_PyTorch(NN, X, y, epoch=10000, optimizer=None):\n","    NN.train()\n","    list_loss = []\n","\n","    X.to(device)\n","    y.to(device)\n","\n","    for i in range(epoch):\n","        # reset optimizer at each epoch\n","        optimizer.zero_grad()\n","\n","        # Process the dataset with the forward pass\n","        yHat = NN.forward(X)\n","\n","        # and compute the loss function\n","        loss_val = NN.loss(yHat, y)\n","\n","        # Automatically compute the gradients\n","        loss_val.backward()\n","        \n","        # Call the optimizer to update the paramters\n","        optimizer.step()\n","\n","        # Print loss and save the value at each iteration\n","        if i%500 == 0:\n","            print('Loss {}={}'.format(i,loss_val))\n","        list_loss.append(loss_val.item())\n","        \n","    print('Loss {}={}'.format(i,loss_val))\n","    return list_loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MLP_PT = MLP_PyTorch()\n","optimizer = torch.optim.SGD(MLP_PT.parameters(), lr=hyp_lr)\n","list_loss = train_PyTorch(MLP_PT, torch.FloatTensor(normalized_train), torch.FloatTensor(train_labels), epoch=hyp_epochs, optimizer=optimizer)\n","\n","plt.plot(list_loss)\n","plt.xlabel('Iterations')\n","plt.ylabel('Loss Val')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(list_loss)\n","plt.title('Pytorch MLP lr=.05')\n","plt.xlabel('Iterations')\n","plt.ylabel('Loss Val')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict_PyTorch(NN, X):\n","    NN.eval()\n","    return torch.round(NN.forward(torch.Tensor(X)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize the decision boundary in 2D of the classification\n","def visualize_boundaries_PyTorch(NN, range_x, range_y,\n","                         norm_train, norm_test, labels_train, labels_test,\n","                         cmap=ListedColormap(['#7E1E9C99', '#FFFF1499']),\n","                         title='MLP'):\n","    \n","    xx, yy = np.meshgrid(np.arange(range_x[0], range_x[1], 0.01), np.arange(range_y[0], range_y[1], 0.01))\n","    input = np.column_stack((xx.flatten(), yy.flatten()))\n","\n","    pred = predict_PyTorch(NN, torch.Tensor(input)).detach().numpy()\n","    pred = pred.reshape(xx.shape)\n","\n","    fig, ax = plt.subplots(1, 2, sharey=True, figsize=(8, 4.5))\n","\n","    ax[0].set_box_aspect(1)\n","    ax[1].set_box_aspect(1)\n","\n","    fig.suptitle('Decision Boundaries ({})'.format(title))\n","    ax[0].set_title('Training dataset')\n","    ax[1].set_title('Test dataset')\n","    ax[0].set_ylabel('Y direction')\n","    ax[0].set_xlabel('X direction')\n","    ax[1].set_xlabel('X direction')\n","\n","    ax[0].pcolormesh(xx, yy, pred, cmap=cmap)\n","    ax[1].pcolormesh(xx, yy, pred, cmap=cmap)\n","\n","    ax[0].scatter(norm_train[:,0], norm_train[:,1], c=labels_train)\n","    ax[1].scatter(norm_test[:,0], norm_test[:,1], c=labels_test)\n","\n","    plt.tight_layout()\n","    plt.show()\n","    # plt.contour([normalized_test[:, 0], normalized_test[:, 1]], )\n","\n","visualize_boundaries_PyTorch(MLP_PT, (-2.5, 2.5), (-2.5, 2.5), normalized_train, normalized_test, train_labels, test_labels, title='MLP on PyTorch')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_yHat = predict_PyTorch(MLP_PT, normalized_test).detach().numpy()\n","train_yHat = predict_PyTorch(MLP_PT, normalized_train).detach().numpy()\n","\n","train_accuracy = accuracy(train_labels, train_yHat)\n","test_accuracy = accuracy(test_labels, test_yHat)\n","\n","print(\"Accuracy obtained in training dataset: %.2f\" % (train_accuracy * 100) + \" %.\")\n","print(\"Accuracy obtained in test dataset: %.2f\" % (test_accuracy * 100) + \" %.\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
