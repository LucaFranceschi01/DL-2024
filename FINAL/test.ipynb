{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H16KuUqpj_T"
      },
      "source": [
        "# **Final Project Application**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L18WKRKwrXIg"
      },
      "source": [
        "Now that we have effectively trained our Doppelganger model, we wish to test its possible applications in a real-case scenario.\n",
        "\n",
        "For that, we wanted to try to leverage the embeddings generated by our trained ResNet model in the latent space to find the closest matching face in our dataset for a given input face image. This approach can have several practical applications, such as:\n",
        "\n",
        "1. **Facial Recognition**: Identifying individuals by comparing their faces to a database of known faces.\n",
        "2. **Doppelganger Finder**: Finding a person in the dataset who looks most similar to a given input image, which can be fun and engaging for users.\n",
        "3. **Data Augmentation**: Generating synthetic datasets by finding lookalikes, which can be useful in training other machine learning models.\n",
        "\n",
        "The key steps involved in implementing this functionality are:\n",
        "\n",
        "*   **Embedding the Entire Dataset**: Using the trained model to generate embeddings (latent space coordinates) for each image in the dataset.\n",
        "*   **Embedding and Comparing**: For a given input face image, generate its embedding and find the closest match by calculating the Euclidean distance to all dataset embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1aDrQthsbVZ"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7WnDLiXsfaQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torchvision.transforms import ToTensor\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from types import MethodType\n",
        "from torchvision.models import mobilenet_v3_small\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPjt6GRr1mpF",
        "outputId": "001aa159-9770-43ac-b5f7-4e5e4331e8e3"
      },
      "outputs": [],
      "source": [
        "# Comment if not in Google Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# data_path = '/content/drive/My Drive/DeepLearning_2024/P4/Data/'\n",
        "# results_path = '/content/drive/My Drive/DeepLearning_2024/P4/Results/'\n",
        "\n",
        "# Comment if in Google Colab\n",
        "data_path = 'data/'\n",
        "results_path = 'results/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uMRRuFH2dSn",
        "outputId": "b64936e8-c017-4cc9-b6e4-71fa010d5fe1"
      },
      "outputs": [],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the dataset instance\n",
        "random_seed = None\n",
        "dataset_version = 'F_'\n",
        "model_name = 'resnet'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIMYQNIXsTtc"
      },
      "source": [
        "## **Dataset definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RANR2UyO5GiV"
      },
      "outputs": [],
      "source": [
        "class HDA_Doppelgaenger(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, datadir = data_path, seed = None, dataset_version = '', transform=None):\n",
        "\n",
        "        self.datadir = datadir\n",
        "        self.transform = transform\n",
        "        self.seed = seed\n",
        "\n",
        "        with open('./' + dataset_version + 'dataset.json', 'r') as fp:\n",
        "            self.indexer = json.load(fp)['HDA_Doppelgaenger']\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        np.random.seed(self.seed)\n",
        "        neg_idx = np.random.randint(0, len(self.indexer)) # index\n",
        "        np.random.seed(self.seed)\n",
        "        neg_type = np.random.choice([0, 1]) # original or lookalike\n",
        "\n",
        "        anchor = Image.open(self.datadir + self.indexer[index][0])\n",
        "        positive = Image.open(self.datadir + self.indexer[index][1])\n",
        "        negative = Image.open(self.datadir + self.indexer[neg_idx][neg_type])\n",
        "\n",
        "        if self.transform is not None :\n",
        "            anchor = self.transform(anchor)\n",
        "            positive = self.transform(positive)\n",
        "            negative = self.transform(negative)\n",
        "\n",
        "        return anchor, positive, negative\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indexer)\n",
        "\n",
        "    def mean(self):\n",
        "        sum = np.zeros(3)\n",
        "        for i in range(len(self)):\n",
        "            sum += np.array(self[i][0]).sum(axis=(0,1))\n",
        "            sum += np.array(self[i][1]).sum(axis=(0,1))\n",
        "        return sum / (len(self)*2) / 255 / 256**2\n",
        "\n",
        "    def mean_std(self):\n",
        "        mean = self.mean()\n",
        "\n",
        "        sum = np.zeros(3)\n",
        "        for i in range(len(self)):\n",
        "            sum += np.power(np.array(self[i][0]), 2).sum(axis=(0,1))\n",
        "            sum += np.power(np.array(self[i][1]), 2).sum(axis=(0,1))\n",
        "        sum = sum / (len(self)*2) / 255 / 256**2\n",
        "\n",
        "        return mean, np.sqrt(sum - np.power(mean, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY_vs9doo41T"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False) # in this one stride = 1 then image shape is kept\n",
        "        self.bn2 = norm_layer(planes)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x): # Tip for improving performance merge all into one line relu(bn2(conv2(relu(bn1(conv1(x))))))\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXaSaacdtKGW"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, layers, num_classes=1000, zero_init_residual=False, norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        # Introduction layer\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Layers\n",
        "        self.layer1 = self._make_layer(64, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(64, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, layers[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Last fully connected layer\n",
        "        # self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        # What is going on ???\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        # if zero_init_residual:\n",
        "        #     for m in self.modules():\n",
        "        #         if isinstance(m, Bottleneck):\n",
        "        #             nn.init.constant_(m.bn3.weight, 0)\n",
        "        #         elif isinstance(m, BasicBlock):\n",
        "        #             nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, in_planes, out_planes, blocks, stride=1):\n",
        "        '''\n",
        "        Creates a layer containing {blocks} ResidualBlocks, where the first one downsamples the output if needed,\n",
        "        and the next {blocks-1} stacked blocks keep the amount of filters unaltered.\n",
        "        '''\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        if stride != 1 or in_planes != out_planes:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
        "                norm_layer(out_planes)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_planes, out_planes, stride, downsample, norm_layer))\n",
        "\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_planes, out_planes, norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _forward_impl2(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        # x = torch.flatten(x, 1)\n",
        "\n",
        "        # x = self.classifier(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLuFYnyl1jFD"
      },
      "source": [
        "## **Loading the pre-trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqWeZj58tNY5"
      },
      "outputs": [],
      "source": [
        "def load_model(checkpoint_path):\n",
        "    pretrained_weights = torch.load(checkpoint_path, weights_only=True, map_location=device)\n",
        "\n",
        "    if model_name=='resnet':\n",
        "        net = ResNet([2, 2, 2, 2]).to(device)\n",
        "        net.load_state_dict(pretrained_weights)\n",
        "        net.fc = None\n",
        "    elif model_name=='mobilenet':\n",
        "        net = mobilenet_v3_small().to(device)\n",
        "        net._forward_impl = MethodType(_forward_impl2, net)\n",
        "        net.load_state_dict(pretrained_weights)\n",
        "    else:\n",
        "        print('Something\\'s wrong...')\n",
        "    return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD-Cjy_51ZW-"
      },
      "outputs": [],
      "source": [
        "net = load_model(results_path + dataset_version + model_name + '.ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxmfcnSy4H57"
      },
      "source": [
        "\n",
        "## **Dataset Embedding**\n",
        "\n",
        "We start by embedding the entire dataset using the trained ResNet model. This involves generating embeddings (latent space coordinates) for each image in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ1o_A-dSLmJ"
      },
      "outputs": [],
      "source": [
        "reduction = 10\n",
        "dataset = HDA_Doppelgaenger(seed=random_seed, dataset_version=dataset_version)\n",
        "\n",
        "HDA_mean, HDA_std = dataset.mean_std()\n",
        "\n",
        "base_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize(HDA_mean, HDA_std)])\n",
        "\n",
        "dataset.transform = base_transform\n",
        "\n",
        "dataset_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                              batch_size=int(len(dataset)/reduction),\n",
        "                                              shuffle=torch._fake_quantize_learnable_per_tensor_affine)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq4sg5w-TXB6"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "To extract embeddings from a specific layer of the pre-trained model, we follow these steps:\n",
        "\n",
        "1. **Select Layer**: We specify the layer from which we want to extract embeddings. In this case, we choose the 'avgpool' layer.\n",
        "\n",
        "2. **Define Embedding Copy Function**: We define a function (`copy_embeddings`) to copy embeddings from the selected layer. Here, we extract embeddings and convert them into NumPy arrays.\n",
        "\n",
        "3. **Attach Hook to the Layer**: We attach a hook to the selected layer. This hook will execute the `copy_embeddings` function every time the forward pass reaches the specified layer.\n",
        "\n",
        "4. **Embedding Function**: We modify the embedding function (`embed_hook`) to use the hook. This function iterates through the dataset, performs forward passes on batches of images, and triggers the hook to extract embeddings.\n",
        "\n",
        "5. **Call the Modified Embedding Function**: We call the modified embedding function (`embed_hook`) with the model, dataset loader, and device as parameters. This process generates embeddings for all images in the dataset.\n",
        "\n",
        "We were inspired by the approach outlined in the ActiveLoop website, specifically by Margaux Masson-Forsythe's work on generating image embeddings using a pre-trained CNN and storing them in Hub.\n",
        "\n",
        "**Source**: *https://www.activeloop.ai/resources/generate-image-embeddings-using-a-pre-trained-cnn-and-store-them-in-hub/*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to copy embeddings from the selected layer\n",
        "def copy_embeddings(m, i, o):\n",
        "    \"\"\"Copy embeddings from the selected layer.\"\"\"\n",
        "    o = o[:, :, 0, 0].detach().cpu().numpy()  # Convert to numpy array\n",
        "    outputs.append(o)\n",
        "\n",
        "# Embedding function modified to use the hook\n",
        "def embed_hook(net, dataloader):\n",
        "    net.eval()\n",
        "    total_batches = len(dataloader)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (anchor, positive, _) in enumerate(dataloader):\n",
        "            anchor = anchor.to(device)\n",
        "            positive = positive.to(device)\n",
        "            _ = net(anchor)  # Forward pass will trigger the hook\n",
        "            _ = net(positive)\n",
        "            print(f\"Iteration: {batch_idx+1}/{total_batches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p4r3N-YR_Vh",
        "outputId": "be16faf7-409c-4387-cd82-cfc54071b566"
      },
      "outputs": [],
      "source": [
        "# Select the desired layer\n",
        "layer = net._modules.get('avgpool')\n",
        "\n",
        "outputs = []\n",
        "\n",
        "# Attach hook to the selected layer\n",
        "_ = layer.register_forward_hook(copy_embeddings)\n",
        "\n",
        "# Call the modified embedding function\n",
        "embed_hook(net, dataset_loader)\n",
        "\n",
        "# Visualize embeddings\n",
        "embeddings = np.concatenate(outputs, axis=0)\n",
        "labels = np.repeat(np.arange(len(dataset)), embeddings.shape[1], axis=0)  # Assuming each image has multiple embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geFUTD-9YP_k"
      },
      "source": [
        "## **Visualizing Embeddings**\n",
        "\n",
        "After embedding the dataset, we visualize the embeddings in 2D and 3D spaces for better understanding and interpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "F9sOIxkW4ZXR",
        "outputId": "8752bc04-8300-4917-e7cf-7c92ec19afee"
      },
      "outputs": [],
      "source": [
        "def visualize_embeddings(embeddings, labels):\n",
        "    # Reshape embeddings to x dimensions\n",
        "    embeddings_2d = embeddings.reshape(embeddings.shape[0], -1)\n",
        "    embeddings_3d = embeddings.reshape(embeddings.shape[0], -1)\n",
        "\n",
        "    # Perform dimensionality reduction with PCA for visualization\n",
        "    pca_2d = PCA(n_components=2)\n",
        "    reduced_embeddings_2d = pca_2d.fit_transform(embeddings_2d)\n",
        "            # 3D visualization\n",
        "    pca_3d = PCA(n_components=3)\n",
        "    reduced_embeddings_3d = pca_3d.fit_transform(embeddings_3d)\n",
        "\n",
        "    # Visualize embeddings in 2D\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    scatter_2d = plt.scatter(reduced_embeddings_2d[:, 0], reduced_embeddings_2d[:, 1], s=10)\n",
        "    plt.title('2D Embeddings')\n",
        "    plt.xlabel('PC 1')\n",
        "    plt.ylabel('PC 2')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Visualize embeddings in 3D\n",
        "    ax = plt.subplot(1, 2, 2, projection='3d')\n",
        "    scatter_3d = ax.scatter(reduced_embeddings_3d[:, 0], reduced_embeddings_3d[:, 1], reduced_embeddings_3d[:, 2], s=10)\n",
        "    ax.set_title('3D Embeddings')\n",
        "    ax.set_xlabel('PC 1')\n",
        "    ax.set_ylabel('PC 2')\n",
        "    ax.set_zlabel('PC 3')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the integrated visualization function\n",
        "visualize_embeddings(embeddings, labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw-2VP5qVjTf"
      },
      "source": [
        "## **Find Doppelganger**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i10iFmcVE4t",
        "outputId": "8f6c355a-20e5-40bb-dc57-b0ccc7edf4cb"
      },
      "outputs": [],
      "source": [
        "def find_doppelganger(model, embeddings, dataset, image, transform=None):\n",
        "    \n",
        "    query_image = image.unsqueeze(0).to(device)\n",
        "\n",
        "    # Obtain the embedding for the query image\n",
        "    with torch.no_grad():\n",
        "        query_embedding = model(query_image)\n",
        "\n",
        "    # Convert query embedding to a NumPy array\n",
        "    query_embedding_array = query_embedding.cpu().detach().numpy()\n",
        "\n",
        "    # Calculate Euclidean distances between the query embedding and all embeddings in the embedding space\n",
        "    distances = np.linalg.norm(embeddings.T - query_embedding_array, axis=0)\n",
        "\n",
        "    # Find the index of the closest embedding\n",
        "    closest_index = np.argmin(distances)\n",
        "\n",
        "    # Retrieve the corresponding image from the dataset\n",
        "    closest_image_path = dataset.indexer[math.floor(closest_index/2)][closest_index%2]\n",
        "\n",
        "    return closest_image_path\n",
        "\n",
        "samples = 5\n",
        "fig, axs = plt.subplots(samples*2-1, 2, sharex=True, sharey=True, figsize=(8, 30))\n",
        "axs[0][0].set_title('Input image')\n",
        "axs[0][1].set_title('Output image')\n",
        "for im in range(samples):\n",
        "    closest_image_name = find_doppelganger(net, embeddings, dataset, dataset[im][0], transform=base_transform)\n",
        "    # print('In: {}, Out: {}'.format(dataset.indexer[im][0], closest_image_name))\n",
        "    axs[im][0].imshow(Image.open(data_path + dataset.indexer[im][0]))\n",
        "    axs[im][0].set_xlabel(dataset.indexer[im][0])\n",
        "    axs[im][1].imshow(Image.open(data_path + closest_image_name))\n",
        "    axs[im][1].set_xlabel(closest_image_name)\n",
        "\n",
        "for im in range(samples):\n",
        "    closest_image_name = find_doppelganger(net, embeddings, dataset, dataset[int(len(dataset)/2)+im][0], transform=base_transform)\n",
        "    # print('In: {}, Out: {}'.format(dataset.indexer[int(len(dataset)/2)+im][0], closest_image_name))\n",
        "    axs[samples-1+im][0].imshow(Image.open(data_path + dataset.indexer[int(len(dataset)/2)+im][0]))\n",
        "    axs[samples-1+im][0].set_xlabel(dataset.indexer[int(len(dataset)/2)+im][0])\n",
        "    axs[samples-1+im][1].imshow(Image.open(data_path + closest_image_name))\n",
        "    axs[samples-1+im][1].set_xlabel(closest_image_name)\n",
        "\n",
        "for ax in axs.flatten():\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
