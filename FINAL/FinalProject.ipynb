{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student 1: Alejandro González Álvarez\n",
    "\n",
    "NIA 1: 252658\n",
    "\n",
    "Student 2: Luca Franceschi\n",
    "\n",
    "NIA 2: 253885\n",
    "\n",
    "Student 3: Júlia Othats-Dalès\n",
    "\n",
    "NIA 3: 254435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclassification of doppelgangers: to which person are you more alike?\\n\\ninput: one facial image\\noutput: closest face in the feature space\\n\\npretrained model for image recognition?\\ntriplet loss?\\nsiamese network?\\n\\nuse transfer learning or train with vggface2?\\nfine tune with smaller dataset of doppelgangers\\n\\ncontrastive learning\\nhttps://encord.com/blog/guide-to-contrastive-learning/\\nhttps://www.v7labs.com/blog/contrastive-learning-guide\\nhttps://arxiv.org/pdf/1512.03385\\nhttps://www.geeksforgeeks.org/residual-networks-resnet-deep-learning/\\nhttps://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\\nhttps://www.v7labs.com/blog/triplet-loss\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ideas\n",
    "\n",
    "'''\n",
    "classification of doppelgangers: to which person are you more alike?\n",
    "\n",
    "input: one facial image\n",
    "output: closest face in the feature space\n",
    "\n",
    "pretrained model for image recognition?\n",
    "triplet loss?\n",
    "siamese network?\n",
    "\n",
    "use transfer learning or train with vggface2?\n",
    "fine tune with smaller dataset of doppelgangers\n",
    "\n",
    "contrastive learning\n",
    "https://encord.com/blog/guide-to-contrastive-learning/\n",
    "https://www.v7labs.com/blog/contrastive-learning-guide\n",
    "https://arxiv.org/pdf/1512.03385\n",
    "https://www.geeksforgeeks.org/residual-networks-resnet-deep-learning/\n",
    "https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n",
    "https://www.v7labs.com/blog/triplet-loss\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html\n",
    "https://medium.com/@Skpd/triplet-loss-on-imagenet-dataset-a2b29b8c2952\n",
    "https://omoindrot.github.io/triplet-loss\n",
    "https://arxiv.org/pdf/1503.03832v3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from torchvision.models import ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment if not in Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# data_path = '/content/drive/My Drive/DeepLearning_2024/P4/Data/'\n",
    "# results_path = '/content/drive/My Drive/DeepLearning_2024/P4/Results/'\n",
    "\n",
    "# Comment if in Google Colab\n",
    "data_path = 'Data/'\n",
    "results_path = 'Results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Print if gpu acceleration is enabled\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and std of the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our re-implementation of the ResNet-18 architecture extracted from [the torchvision github repository](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py). We believe that it is important to note that it is not a simple copy-paste, we have put our best efforts in understanding and re-implementing it to fufill our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ResNet building block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False) # in this one stride = 1 then image shape is kept\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x): # Tip for improving performance merge all into one line relu(bn2(conv2(relu(bn1(conv1(x))))))\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, num_classes=1000, zero_init_residual=False, norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        # Introduction layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Layers\n",
    "        self.layer1 = self._make_layer(64, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(64, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Last fully connected layer\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "        # What is going on ???\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        # if zero_init_residual:\n",
    "        #     for m in self.modules():\n",
    "        #         if isinstance(m, Bottleneck):\n",
    "        #             nn.init.constant_(m.bn3.weight, 0)\n",
    "        #         elif isinstance(m, BasicBlock):\n",
    "        #             nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, in_planes, out_planes, blocks, stride=1):\n",
    "        '''\n",
    "        Creates a layer containing {blocks} ResidualBlocks, where the first one downsamples the output if needed,\n",
    "        and the next {blocks-1} stacked blocks keep the amount of filters unaltered.\n",
    "        '''\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                norm_layer(out_planes)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_planes, out_planes, stride, downsample, norm_layer))\n",
    "\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_planes, out_planes, norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [128, 1000]               --\n",
       "├─Conv2d: 1-1                            [128, 64, 128, 128]       9,408\n",
       "├─BatchNorm2d: 1-2                       [128, 64, 128, 128]       128\n",
       "├─ReLU: 1-3                              [128, 64, 128, 128]       --\n",
       "├─MaxPool2d: 1-4                         [128, 64, 64, 64]         --\n",
       "├─Sequential: 1-5                        [128, 64, 64, 64]         --\n",
       "│    └─ResidualBlock: 2-1                [128, 64, 64, 64]         --\n",
       "│    │    └─Conv2d: 3-1                  [128, 64, 64, 64]         36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [128, 64, 64, 64]         128\n",
       "│    │    └─ReLU: 3-3                    [128, 64, 64, 64]         --\n",
       "│    │    └─Conv2d: 3-4                  [128, 64, 64, 64]         36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [128, 64, 64, 64]         128\n",
       "│    │    └─ReLU: 3-6                    [128, 64, 64, 64]         --\n",
       "│    └─ResidualBlock: 2-2                [128, 64, 64, 64]         --\n",
       "│    │    └─Conv2d: 3-7                  [128, 64, 64, 64]         36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [128, 64, 64, 64]         128\n",
       "│    │    └─ReLU: 3-9                    [128, 64, 64, 64]         --\n",
       "│    │    └─Conv2d: 3-10                 [128, 64, 64, 64]         36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [128, 64, 64, 64]         128\n",
       "│    │    └─ReLU: 3-12                   [128, 64, 64, 64]         --\n",
       "├─Sequential: 1-6                        [128, 128, 32, 32]        --\n",
       "│    └─ResidualBlock: 2-3                [128, 128, 32, 32]        --\n",
       "│    │    └─Conv2d: 3-13                 [128, 128, 32, 32]        73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [128, 128, 32, 32]        256\n",
       "│    │    └─ReLU: 3-15                   [128, 128, 32, 32]        --\n",
       "│    │    └─Conv2d: 3-16                 [128, 128, 32, 32]        147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [128, 128, 32, 32]        256\n",
       "│    │    └─Sequential: 3-18             [128, 128, 32, 32]        8,448\n",
       "│    │    └─ReLU: 3-19                   [128, 128, 32, 32]        --\n",
       "│    └─ResidualBlock: 2-4                [128, 128, 32, 32]        --\n",
       "│    │    └─Conv2d: 3-20                 [128, 128, 32, 32]        147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [128, 128, 32, 32]        256\n",
       "│    │    └─ReLU: 3-22                   [128, 128, 32, 32]        --\n",
       "│    │    └─Conv2d: 3-23                 [128, 128, 32, 32]        147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [128, 128, 32, 32]        256\n",
       "│    │    └─ReLU: 3-25                   [128, 128, 32, 32]        --\n",
       "├─Sequential: 1-7                        [128, 256, 16, 16]        --\n",
       "│    └─ResidualBlock: 2-5                [128, 256, 16, 16]        --\n",
       "│    │    └─Conv2d: 3-26                 [128, 256, 16, 16]        294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [128, 256, 16, 16]        512\n",
       "│    │    └─ReLU: 3-28                   [128, 256, 16, 16]        --\n",
       "│    │    └─Conv2d: 3-29                 [128, 256, 16, 16]        589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [128, 256, 16, 16]        512\n",
       "│    │    └─Sequential: 3-31             [128, 256, 16, 16]        33,280\n",
       "│    │    └─ReLU: 3-32                   [128, 256, 16, 16]        --\n",
       "│    └─ResidualBlock: 2-6                [128, 256, 16, 16]        --\n",
       "│    │    └─Conv2d: 3-33                 [128, 256, 16, 16]        589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [128, 256, 16, 16]        512\n",
       "│    │    └─ReLU: 3-35                   [128, 256, 16, 16]        --\n",
       "│    │    └─Conv2d: 3-36                 [128, 256, 16, 16]        589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [128, 256, 16, 16]        512\n",
       "│    │    └─ReLU: 3-38                   [128, 256, 16, 16]        --\n",
       "├─Sequential: 1-8                        [128, 512, 8, 8]          --\n",
       "│    └─ResidualBlock: 2-7                [128, 512, 8, 8]          --\n",
       "│    │    └─Conv2d: 3-39                 [128, 512, 8, 8]          1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [128, 512, 8, 8]          1,024\n",
       "│    │    └─ReLU: 3-41                   [128, 512, 8, 8]          --\n",
       "│    │    └─Conv2d: 3-42                 [128, 512, 8, 8]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [128, 512, 8, 8]          1,024\n",
       "│    │    └─Sequential: 3-44             [128, 512, 8, 8]          132,096\n",
       "│    │    └─ReLU: 3-45                   [128, 512, 8, 8]          --\n",
       "│    └─ResidualBlock: 2-8                [128, 512, 8, 8]          --\n",
       "│    │    └─Conv2d: 3-46                 [128, 512, 8, 8]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [128, 512, 8, 8]          1,024\n",
       "│    │    └─ReLU: 3-48                   [128, 512, 8, 8]          --\n",
       "│    │    └─Conv2d: 3-49                 [128, 512, 8, 8]          2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [128, 512, 8, 8]          1,024\n",
       "│    │    └─ReLU: 3-51                   [128, 512, 8, 8]          --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [128, 512, 1, 1]          --\n",
       "├─Linear: 1-10                           [128, 1000]               513,000\n",
       "==========================================================================================\n",
       "Total params: 11,689,512\n",
       "Trainable params: 11,689,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 303.26\n",
       "==========================================================================================\n",
       "Input size (MB): 100.66\n",
       "Forward/backward pass size (MB): 6644.80\n",
       "Params size (MB): 46.76\n",
       "Estimated Total Size (MB): 6792.22\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(ResNet([2, 2, 2, 2], 1000), input_size=(128,3,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How pretrained weights are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\alex4/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:02<00:00, 15.9MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_weights = ResNet18_Weights.DEFAULT.get_state_dict()\n",
    "net = ResNet([2, 2, 2, 2])\n",
    "net.load_state_dict(pretrained_weights)\n",
    "# remove fc layer\n",
    "# fishernet vgg16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, val_loader):\n",
    "    net.eval()\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    criterion = nn.TripletMarginLoss()\n",
    "\n",
    "    val_loss_epoch = 0\n",
    "\n",
    "    for anchors, positives, negatives in val_loader: # batches of positive, negative and anchor images\n",
    "\n",
    "        anchors = anchors.to(device)\n",
    "        positives = positives.to(device)\n",
    "        negatives = negatives.to(device)\n",
    "\n",
    "        out_anc = net(anchors)\n",
    "        out_pos = net(positives)\n",
    "        out_neg = net(negatives)\n",
    "\n",
    "        loss = criterion(out_anc, out_pos, out_neg)\n",
    "\n",
    "        val_loss_epoch += loss.cpu().item()\n",
    "        \n",
    "        loss_list.append(loss.cpu().item())\n",
    "\n",
    "    return val_loss_epoch / len(val_loader), loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(net, tr_loader, val_loader, epochs=10, optimizer=None, model_name='default.ckpt'):\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    loss_list = {\n",
    "        'training': [],\n",
    "        'validation': []\n",
    "    }\n",
    "\n",
    "    criterion = nn.TripletMarginLoss()\n",
    "\n",
    "    early_stopper = EarlyStopper(patience=3, min_delta=10)\n",
    "\n",
    "    for e in range(0, epochs):\n",
    "\n",
    "        loss_list['training'].append([])\n",
    "        loss_list['validation'].append([])\n",
    "\n",
    "        tr_loss_epoch = 0\n",
    "\n",
    "        for anchors, positives, negatives in tr_loader: # batches of positive, negative and anchor images\n",
    "\n",
    "            anchors = anchors.to(device)\n",
    "            positives = positives.to(device)\n",
    "            negatives = negatives.to(device)\n",
    "\n",
    "            out_anc = net(anchors)\n",
    "            out_pos = net(positives)\n",
    "            out_neg = net(negatives)\n",
    "\n",
    "            loss = criterion(out_anc, out_pos, out_neg)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss_epoch += loss.cpu().item()\n",
    "            \n",
    "            loss_list['training'][e].append(loss.cpu().item())\n",
    "\n",
    "        tr_loss_epoch /= len(tr_loader)\n",
    "\n",
    "        val_loss_epoch, val_loss_list = validate(net, val_loader)\n",
    "        loss_list['validation'][e].append(val_loss_list)\n",
    "\n",
    "        print('Epoch [{:4d}/{:4d}] | Train Loss: {:.4f} | Validation Loss: {:4d}'.format(e+1, epochs, tr_loss_epoch, val_loss_epoch))\n",
    "        \n",
    "        # save trained model\n",
    "        torch.save(net.state_dict(), results_path + '/' + model_name)\n",
    "\n",
    "        if early_stopper.early_stop(val_loss_epoch):\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
