{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student 1: Alejandro González Álvarez\n",
    "\n",
    "NIA 1: 252658\n",
    "\n",
    "Student 2: Luca Franceschi\n",
    "\n",
    "NIA 2: 253885\n",
    "\n",
    "Student 3: Júlia Othats-Dalès\n",
    "\n",
    "NIA 3: 254435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideas\n",
    "\n",
    "'''\n",
    "classification of doppelgangers: to which person are you more alike?\n",
    "\n",
    "input: one facial image\n",
    "output: closest face in the feature space\n",
    "\n",
    "siamese network?\n",
    "\n",
    "use transfer learning or train with vggface2?\n",
    "fine tune with smaller dataset of doppelgangers\n",
    "\n",
    "contrastive learning\n",
    "https://encord.com/blog/guide-to-contrastive-learning/\n",
    "https://www.v7labs.com/blog/contrastive-learning-guide\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import umap\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import scipy.io as sio\n",
    "from types import MethodType\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet18_Weights, mobilenet_v3_small, MobileNet_V3_Small_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment if not in Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# data_path = '/content/drive/My Drive/DeepLearning_2024/P4/Data/'\n",
    "# results_path = '/content/drive/My Drive/DeepLearning_2024/P4/Results/'\n",
    "\n",
    "# Comment if in Google Colab\n",
    "data_path = 'data/'\n",
    "results_path = 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print if gpu acceleration is enabled\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_seed = 42\n",
    "dataset_version = 'F_dataset.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The HDA Doppelgaenger Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://discuss.pytorch.org/t/torch-utils-data-dataset-random-split/32209/4\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDA_Doppelgaenger(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, datadir = data_path, dataset_version = 'dataset.json', seed = None, transform=None):\n",
    "\n",
    "        self.datadir = datadir\n",
    "        self.transform = transform\n",
    "        if seed != None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        with open('./' + dataset_version, 'r') as fp:\n",
    "            self.indexer = json.load(fp)['HDA_Doppelgaenger']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        neg_idx = np.random.randint(0, len(self.indexer)) # index\n",
    "        neg_type = np.random.choice([0, 1]) # original or lookalike\n",
    "\n",
    "        anchor = Image.open(self.datadir + self.indexer[index][0])\n",
    "        positive = Image.open(self.datadir + self.indexer[index][1])\n",
    "        negative = Image.open(self.datadir + self.indexer[neg_idx][neg_type])\n",
    "\n",
    "        if self.transform is not None :\n",
    "            anchor = self.transform(anchor)\n",
    "            positive = self.transform(positive)\n",
    "            negative = self.transform(negative)\n",
    "\n",
    "        return anchor, positive, negative\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indexer)\n",
    "    \n",
    "    def mean(self):\n",
    "        sum = np.zeros(3)\n",
    "        for i in range(len(self)):\n",
    "            sum += np.array(self[i][0]).sum(axis=(0,1))\n",
    "            sum += np.array(self[i][1]).sum(axis=(0,1))\n",
    "        return sum / (len(self)*2) / 255 / 256**2\n",
    "    \n",
    "    def mean_std(self):\n",
    "        mean = self.mean()\n",
    "\n",
    "        sum = np.zeros(3)\n",
    "        for i in range(len(self)):\n",
    "            sum += np.power(np.array(self[i][0]), 2).sum(axis=(0,1))\n",
    "            sum += np.power(np.array(self[i][1]), 2).sum(axis=(0,1))\n",
    "        sum = sum / (len(self)*2) / 255 / 256**2\n",
    "        \n",
    "        return mean, np.sqrt(sum - np.power(mean, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDA_Subset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        a, p, n = self.subset[index]\n",
    "        if self.transform:\n",
    "            a = self.transform(a)\n",
    "            p = self.transform(p)\n",
    "            n = self.transform(n)\n",
    "        return a, p, n\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HDA_Doppelgaenger(dataset_version=dataset_version) # !!! if seed is used, accuracy in test function will not work as expected\n",
    "\n",
    "HDA_mean, HDA_std = dataset.mean_std()\n",
    "\n",
    "base_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(HDA_mean, HDA_std)])\n",
    "\n",
    "augm_transform = transforms.Compose(\n",
    "     [transforms.RandomHorizontalFlip(0.5),\n",
    "     transforms.RandomGrayscale(0.2),\n",
    "     transforms.ColorJitter(0.5, 0.5, 0.5, 0.5),\n",
    "     transforms.RandomAffine(45),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(HDA_mean, HDA_std)])\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, sharex=True, sharey=True)\n",
    "\n",
    "axs[0].imshow(dataset[0][0])\n",
    "axs[1].imshow(dataset[0][1])\n",
    "axs[2].imshow(dataset[0][2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 10 # the batches' size will be = len(dataset) / reduction\n",
    "\n",
    "generator = torch.Generator().manual_seed(fixed_seed) # to have reproducible results\n",
    "train, validation, test = random_split(dataset, [0.85, 0.1, 0.05], generator=generator)\n",
    "\n",
    "train = HDA_Subset(train, augm_transform)\n",
    "validation = HDA_Subset(validation, base_transform)\n",
    "test = HDA_Subset(test, base_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=int(len(train)/reduction), shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation, batch_size=int(len(validation)/reduction), shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=int(len(test)/reduction), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toimage = transforms.ToPILImage()\n",
    "toimage(validation[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose ONE architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tried several architectures, our recommendation is to run one at a time so there are no confusions. The easiest way to do that is to use the feature \"Run Cells in section\" from a single architecture at a time. After each training and testing execution models and training losses are automatically saved based on which is the last architecture selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our re-implementation of the ResNet-18 architecture extracted from [the torchvision github repository](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py). We believe that it is important to note that it is not a simple copy-paste, we have put our best efforts in understanding and re-implementing it to fufill our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic ResNet building block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False) # in this one stride = 1 then image shape is kept\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x): # Tip for improving performance merge all into one line relu(bn2(conv2(relu(bn1(conv1(x))))))\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://arxiv.org/pdf/1512.03385\n",
    "https://www.geeksforgeeks.org/residual-networks-resnet-deep-learning/\n",
    "https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, num_classes=1000, zero_init_residual=False, norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        # Introduction layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Layers\n",
    "        self.layer1 = self._make_layer(64, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(64, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Last fully connected layer\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "        # What is going on ???\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        # if zero_init_residual:\n",
    "        #     for m in self.modules():\n",
    "        #         if isinstance(m, Bottleneck):\n",
    "        #             nn.init.constant_(m.bn3.weight, 0)\n",
    "        #         elif isinstance(m, BasicBlock):\n",
    "        #             nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, in_planes, out_planes, blocks, stride=1):\n",
    "        '''\n",
    "        Creates a layer containing {blocks} ResidualBlocks, where the first one downsamples the output if needed,\n",
    "        and the next {blocks-1} stacked blocks keep the amount of filters unaltered.\n",
    "        '''\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                norm_layer(out_planes)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_planes, out_planes, stride, downsample, norm_layer))\n",
    "\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_planes, out_planes, norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(ResNet([2, 2, 2, 2], 1000), input_size=(128,3,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How pretrained weights are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = ResNet18_Weights.DEFAULT.get_state_dict()\n",
    "net = ResNet([2, 2, 2, 2]).to(device)\n",
    "net.load_state_dict(pretrained_weights)\n",
    "# TRY WITH fishernet vgg16\n",
    "net.fc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://arxiv.org/pdf/1409.1556\n",
    "\n",
    "TOO MANY PARAMS 135M LOL no way this works on a 800 sample dataset\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet V3 (small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://arxiv.org/pdf/1905.02244\n",
    "https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv3.py\n",
    "https://stackoverflow.com/a/73545468/20764091\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet V3 Architecture changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = mobilenet_v3_small().to(device)\n",
    "# monkey patching forward method to repurpose this network\n",
    "def _forward_impl2(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # x = torch.flatten(x, 1)\n",
    "\n",
    "        # x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "net._forward_impl = MethodType(_forward_impl2, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(net, input_size=(128,3,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How pretrained weights are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = MobileNet_V3_Small_Weights.DEFAULT.get_state_dict()\n",
    "net.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mobilenet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://stackoverflow.com/a/73704579/20764091\n",
    "https://www.v7labs.com/blog/triplet-loss\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html\n",
    "https://medium.com/@Skpd/triplet-loss-on-imagenet-dataset-a2b29b8c2952\n",
    "https://omoindrot.github.io/triplet-loss\n",
    "https://arxiv.org/pdf/1503.03832v3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, net, validation_loss, model_name):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "\n",
    "            # save trained model\n",
    "            torch.save(net.state_dict(), results_path + model_name + '.ckpt')\n",
    "\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, val_loader):\n",
    "    net.eval()\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    criterion = nn.TripletMarginLoss()\n",
    "\n",
    "    val_loss_epoch = 0\n",
    "\n",
    "    for anchors, positives, negatives in val_loader: # batches of positive, negative and anchor images\n",
    "\n",
    "        anchors = anchors.to(device)\n",
    "        positives = positives.to(device)\n",
    "        negatives = negatives.to(device)\n",
    "\n",
    "        out_anc = net(anchors)\n",
    "        out_pos = net(positives)\n",
    "        out_neg = net(negatives)\n",
    "\n",
    "        loss = criterion(out_anc, out_pos, out_neg)\n",
    "\n",
    "        val_loss_epoch += loss.cpu().item()\n",
    "        \n",
    "        loss_list.append(loss.cpu().item())\n",
    "\n",
    "    return val_loss_epoch / len(val_loader), loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(net, tr_loader, val_loader, epochs=10, optimizer=None, model_name='default', early_stopper_tolerance=0.1):\n",
    "    \n",
    "    loss_list = {\n",
    "        'training': [],\n",
    "        'validation': []\n",
    "    }\n",
    "\n",
    "    criterion = nn.TripletMarginLoss()\n",
    "\n",
    "    early_stopper = EarlyStopper(patience=3, min_delta=early_stopper_tolerance)\n",
    "\n",
    "    for e in range(0, epochs):\n",
    "\n",
    "        net.train()\n",
    "\n",
    "        loss_list['training'].append([])\n",
    "        loss_list['validation'].append([])\n",
    "\n",
    "        tr_loss_epoch = 0\n",
    "\n",
    "        for anchors, positives, negatives in tr_loader: # batches of positive, negative and anchor images\n",
    "\n",
    "            anchors = anchors.to(device)\n",
    "            positives = positives.to(device)\n",
    "            negatives = negatives.to(device)\n",
    "\n",
    "            out_anc = net(anchors)\n",
    "            out_pos = net(positives)\n",
    "            out_neg = net(negatives)\n",
    "\n",
    "            loss = criterion(out_anc, out_pos, out_neg)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss_epoch += loss.cpu().item()\n",
    "            \n",
    "            loss_list['training'][e].append(loss.cpu().item())\n",
    "\n",
    "        tr_loss_epoch /= len(tr_loader)\n",
    "\n",
    "        val_loss_epoch, val_loss_list = validate(net, val_loader)\n",
    "        loss_list['validation'][e] = val_loss_list\n",
    "\n",
    "        print('Epoch [{:4n}/{:4n}]  |  Train Loss: {:.5f}  |  Validation Loss: {:.5f}'.format(e+1, epochs, tr_loss_epoch, val_loss_epoch))\n",
    "\n",
    "        if early_stopper.early_stop(net, val_loss_epoch, model_name):\n",
    "            return loss_list\n",
    "        \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(net, test_loader, verbose=False):\n",
    "    correct = wrong = 0\n",
    "    mean_near = mean_far = 0\n",
    "\n",
    "    for anchor, positive, negative in test_loader: # batches of positive, negative and anchor images\n",
    "\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "\n",
    "        out_anc = net(anchor).flatten().detach().cpu().numpy()\n",
    "        out_pos = net(positive).flatten().detach().cpu().numpy()\n",
    "        out_neg = net(negative).flatten().detach().cpu().numpy()\n",
    "\n",
    "        near = np.linalg.norm(out_anc - out_pos, 2)\n",
    "        far = np.linalg.norm(out_anc - out_neg, 2)\n",
    "\n",
    "        mean_near += near\n",
    "        mean_far += far\n",
    "\n",
    "        if far > near:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "\n",
    "        if verbose:\n",
    "            print('D: {:>8.4f}  |  ND: {:>8.4f}'.format(near, far))\n",
    "\n",
    "    return correct, wrong, mean_near, mean_far\n",
    "\n",
    "def test(net, test_loader):\n",
    "\n",
    "    print('{:=^28}'.format(' Losses '))\n",
    "\n",
    "    correct, wrong, mean_near, mean_far = test_epoch(net, test_loader, verbose=True)\n",
    "\n",
    "    # TODO: PRINT IMAGES, LATENT SPACE, ETC --> BASICALLY TEST IS FOR VISUALIZATION OF SPECIFIC RESULTS\n",
    "\n",
    "    print('{:-^28}'.format(' Mean '))\n",
    "    print('D: {:>8.4f}  |  ND: {:>8.4f}'.format(mean_near/len(test_loader), mean_far/len(test_loader)))\n",
    "\n",
    "    print('\\n{:=^28}'.format(' More stats '))\n",
    "\n",
    "    epochs = 49 # one done before\n",
    "    for _ in range(epochs):\n",
    "        c, w, n, f = test_epoch(net, test_loader)\n",
    "        correct += c\n",
    "        wrong += w\n",
    "        mean_near += n\n",
    "        mean_far += f\n",
    "\n",
    "    print('Accuracy: {:>18.2%}'.format(correct/((correct+wrong))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, weight_decay=1e-5)\n",
    "list_loss = fit(net, train_loader, validation_loader, 20, optimizer=optimizer, model_name=model_name)\n",
    "\n",
    "sio.savemat(results_path + 'll_' + model_name + '.mat', list_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reloading models before testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMEMBER BEFORE TESTING!\n",
    "\n",
    "RELOAD THE MODEL FROM THE CHECKPOINT, OTHERWISE THE TESTING WILL BE SUBOPTIMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = torch.load(results_path + model_name + '.ckpt', weights_only=True, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name=='resnet':\n",
    "    net = ResNet([2, 2, 2, 2]).to(device)\n",
    "    net.fc = None\n",
    "    net.load_state_dict(pretrained_weights)\n",
    "elif model_name=='mobilenet':\n",
    "    net = mobilenet_v3_small().to(device)\n",
    "    net._forward_impl = MethodType(_forward_impl2, net)\n",
    "    net.load_state_dict(pretrained_weights)\n",
    "else:\n",
    "    print('Something\\'s wrong...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who is your Doppelgaenger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea: embed all dataset, then if we want to get the nearest face in the latent space embed a face and argmin euclidean distances\n",
    "def embed(net, dataloader):\n",
    "    pass\n",
    "\n",
    "def embed_and_compare(net, face, embeddings):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedDict = {}\n",
    "sio.loadmat(results_path + 'll_' + model_name + '.mat', loadedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.array(loadedDict['training']).mean(axis=0), linestyle='-.', color='r', label='Training')\n",
    "plt.plot(np.array(loadedDict['validation']).mean(axis=0), linestyle='-', color='b', label='Validation')\n",
    "plt.title('Training Loss {} lr={}'.format(model_name, learning_rate))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.array(loadedDict['training']).flatten(), linestyle='-.', color='r', label='Training')\n",
    "plt.plot(np.array(loadedDict['validation']).flatten(), linestyle='-', color='b', label='Validation')\n",
    "plt.title('Training Loss {} lr={}'.format(model_name, learning_rate))\n",
    "plt.xlabel('Batches seen')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_tr = HDA_Doppelgaenger(dataset_version=dataset_version, transform=base_transform)\n",
    "\n",
    "# mapper = umap.UMAP().fit(dataset_tr) # CANNOT FIND 2D VISUALIZATION !!!\n",
    "# umap.plot.points(mapper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
